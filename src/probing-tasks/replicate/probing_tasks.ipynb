{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "probing_tasks.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "interpreter": {
      "hash": "b6f5aa6c83ab6bb418a26169cae6e1f2aa26236cb3670b5bbe663715684b9fc3"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.10 64-bit"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Check if the TPU is available (only if TPU was selected as hardware accelerator)."
      ],
      "metadata": {
        "id": "apnbrYHb2O8E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "source": [
        "import os\n",
        "assert os.environ['COLAB_TPU_ADDR'], 'Make sure to select TPU from Edit > Notebook settings > Hardware accelerator'"
      ],
      "outputs": [],
      "metadata": {
        "id": "E0dkWt23SPlM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Installation:**\n",
        "Clone our repository and to get the python scripts and the datasets for probing."
      ],
      "metadata": {
        "id": "2VrmlVkoL-e4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!git clone https://github.com/SwiftPredator/How-Does-Bert-Answer-QA-DLP2021/\n",
        "!mv \"/content/How-Does-Bert-Answer-QA-DLP2021/src/probing-tasks/data\" \"/content/\"\n",
        "!mv \"/content/How-Does-Bert-Answer-QA-DLP2021/src/probing-tasks/replicate\" \"/content/\"\n",
        "\n",
        "%cd /content/replicate\n",
        "!pip install -r requirements.txt\n",
        "!pip install cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.9-cp37-cp37m-linux_x86_64.whl"
      ],
      "outputs": [],
      "metadata": {
        "id": "b1iwoQwxL-e4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Restart the runtime and import the libraries. If the runtime is restarted at any point, it suffices to run from here on."
      ],
      "metadata": {
        "id": "DEEBpewv2qYU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "%cd /content/replicate\n",
        "\n",
        "from edge_probing_utils import (\n",
        "    JiantDatasetSingleSpan,\n",
        "    JiantDatasetTwoSpan\n",
        "    )\n",
        "\n",
        "import edge_probing as ep\n",
        "import edge_probing_tpu as ep_tpu\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data as data\n",
        "from transformers import AutoModel, AutoTokenizer"
      ],
      "outputs": [],
      "metadata": {
        "id": "RXkn3b1NNzCQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Setup:**\n",
        "Select the tasks and models to run and pick a dataset size. Since the time for a probing tasks depends on the patience, patience_lr and max_evals parameters in the ProbeConfig, it is adviced to always select the big dataset."
      ],
      "metadata": {
        "id": "3sFwogycL-e4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "\n",
        "tasks = [\n",
        "    #\"ner\", \n",
        "    #\"semeval\",\n",
        "    \"coref\",\n",
        "    \"sup-squad\",\n",
        "    \"ques\",\n",
        "    #\"sup-babi\",\n",
        "    \"adversarialqa\",\n",
        "    ]\n",
        "\n",
        "task_types = {\n",
        "    \"ner\": \"single_span\", \n",
        "    \"semeval\": \"single_span\",\n",
        "    \"coref\": \"two_span\",\n",
        "    \"sup-squad\": \"two_span\",\n",
        "    \"ques\": \"single_span\",\n",
        "    \"sup-babi\": \"two_span\",\n",
        "    \"adversarialqa\": \"two_span\",\n",
        "    }\n",
        "\n",
        "models = [\n",
        "    \"roberta-base\",\n",
        "    #\"bert-base-uncased\",\n",
        "    #\"csarron/bert-base-uncased-squad-v1\"\n",
        "    #\"/content/drive/MyDrive/BERT_fineTunned_on_AdversarialQA_trained\"\n",
        "    ]\n",
        "\n",
        "task_labels_to_ids = {\n",
        "    \"ner\": {'ORDINAL': 0, 'DATE': 1, 'PERSON': 2, 'LOC': 3, 'GPE': 4, 'QUANTITY': 5, 'ORG': 6, 'WORK_OF_ART': 7, 'CARDINAL': 8, 'TIME': 9, 'MONEY': 10, 'LANGUAGE': 11, 'NORP': 12, 'PERCENT': 13, 'EVENT': 14, 'LAW': 15, 'FAC': 16, 'PRODUCT': 17},\n",
        "    \"coref\": {\"0\": 0, \"1\": 1},\n",
        "    \"semeval\": {'Component-Whole(e2,e1)': 0, 'Other': 1, 'Instrument-Agency(e2,e1)': 2, 'Member-Collection(e1,e2)': 3, 'Entity-Destination(e1,e2)': 4, 'Content-Container(e1,e2)': 5, 'Message-Topic(e1,e2)': 6, 'Cause-Effect(e2,e1)': 7, 'Product-Producer(e2,e1)': 8, 'Member-Collection(e2,e1)': 9, 'Entity-Origin(e1,e2)': 10, 'Cause-Effect(e1,e2)': 11, 'Component-Whole(e1,e2)': 12, 'Message-Topic(e2,e1)': 13, 'Product-Producer(e1,e2)': 14, 'Entity-Origin(e2,e1)': 15, 'Content-Container(e2,e1)': 16, 'Instrument-Agency(e1,e2)': 17, 'Entity-Destination(e2,e1)': 18},\n",
        "    \"sup-squad\": {\"0\": 0, \"1\": 1},\n",
        "    \"ques\": {'LOC:other': 0, 'DESC:desc': 1, 'DESC:def': 2, 'DESC:manner': 3, 'ENTY:sport': 4, 'ENTY:termeq': 5, 'HUM:ind': 6, 'NUM:count': 7, 'DESC:reason': 8, 'LOC:country': 9, 'HUM:desc': 10, 'ENTY:animal': 11, 'ENTY:other': 12, 'LOC:city': 13, 'ENTY:cremat': 14, 'NUM:perc': 15, 'NUM:money': 16, 'NUM:date': 17, 'ENTY:dismed': 18, 'LOC:state': 19, 'NUM:speed': 20, 'HUM:gr': 21, 'NUM:dist': 22, 'ENTY:food': 23, 'ABBR:abb': 24, 'ENTY:product': 25, 'HUM:title': 26, 'NUM:weight': 27, 'ABBR:exp': 28, 'ENTY:veh': 29, 'NUM:period': 30, 'ENTY:religion': 31, 'ENTY:letter': 32, 'ENTY:color': 33, 'ENTY:body': 34, 'ENTY:event': 35, 'ENTY:substance': 36, 'ENTY:instru': 37, 'ENTY:plant': 38, 'ENTY:symbol': 39, 'NUM:other': 40, 'LOC:mount': 41, 'NUM:temp': 42, 'ENTY:techmeth': 43, 'NUM:code': 44, 'ENTY:word': 45, 'ENTY:lang': 46, 'NUM:volsize': 47, 'NUM:ord': 48, 'ENTY:currency': 49},\n",
        "    \"sup-babi\": {\"0\": 0, \"1\": 1},\n",
        "    \"adversarialqa\": {\"0\": 0, \"1\": 1},\n",
        "    }\n",
        "\n",
        "# Pick a dataset size: small, medium, big\n",
        "size = \"big\""
      ],
      "outputs": [],
      "metadata": {
        "id": "C0WunHIAL-e4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Run:**\n",
        "Run the selected probing tasks. Select the layers to probe and the device. If another device than the TPU is used make sure to comment out the first two imports (torch_xla and torch_xla.core.xla_model) and set tpu to False. Furthermore the import edge_probing_tpu two cells above needs to be commented out.\n",
        "\n",
        "The results of the run are saved to /content/drive/MyDrive/replicated/probing-results/{size}.json. This file is updated everytime a task was probed on all selected layers. If for some reason the notebook fails to probe all layers, the intermediate results of the last task can be found at the specified results_path (by default /content/drive/MyDrive/replicated/intermediate-results/results.json).\n",
        "\n",
        "The probing for the report was done with different big datasets (the size was adjusted afterwards and they were shuffled). To reproduce those results set original to True. Note that the dataset size has to be set to big for this."
      ],
      "metadata": {
        "id": "CpqnGg-K1SfU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import torch_xla\n",
        "import torch_xla.core.xla_model as xm\n",
        "import os\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "loss_function = nn.BCELoss()\n",
        "batch_size = 32\n",
        "num_layers = range(1,13,2)\n",
        "num_workers = 0\n",
        "\n",
        "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device = xm.xla_device()\n",
        "tpu = True\n",
        "#device = torch.device(\"cpu\")\n",
        "\n",
        "original = False\n",
        "\n",
        "# Disable warnings.\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "os.makedirs(f\"/content/drive/MyDrive/replicated/probing-results/\", exist_ok=True)\n",
        "os.makedirs(f\"/content/drive/MyDrive/replicated/intermediate-results/\", exist_ok=True)\n",
        "\n",
        "for model in models:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model)\n",
        "    for task in tasks:\n",
        "        if os.path.isfile(f'/content/drive/MyDrive/replicated/probing-results/{size}.json'):\n",
        "            with open(f'/content/drive/MyDrive/replicated/probing-results/{size}.json', 'r') as f:\n",
        "                results = json.load(f)\n",
        "        else:\n",
        "            results = {}\n",
        "        task_results = results.setdefault(task, {})\n",
        "        labels_to_ids = task_labels_to_ids[task]\n",
        "        if original:\n",
        "            train_data = ep.tokenize_jiant_dataset(\n",
        "                tokenizer,\n",
        "                *(ep.read_jiant_dataset(f\"/content/data/{task}/{size}/original/train.jsonl\")),\n",
        "                labels_to_ids,\n",
        "                )\n",
        "            val_data = ep.tokenize_jiant_dataset(\n",
        "                tokenizer,\n",
        "                *(ep.read_jiant_dataset(f\"/content/data/{task}/{size}/original/val.jsonl\")),\n",
        "                labels_to_ids,\n",
        "                )\n",
        "            test_data = ep.tokenize_jiant_dataset(\n",
        "                tokenizer,\n",
        "                *(ep.read_jiant_dataset(f\"/content/data/{task}/{size}/original/val.jsonl\")),\n",
        "                labels_to_ids,\n",
        "                )\n",
        "        else:\n",
        "            train_data = ep.tokenize_jiant_dataset(\n",
        "                tokenizer,\n",
        "                *(ep.read_jiant_dataset(f\"/content/data/{task}/{size}/train.jsonl\")),\n",
        "                labels_to_ids,\n",
        "                )\n",
        "            val_data = ep.tokenize_jiant_dataset(\n",
        "                tokenizer,\n",
        "                *(ep.read_jiant_dataset(f\"/content/data/{task}/{size}/val.jsonl\")),\n",
        "                labels_to_ids,\n",
        "                )\n",
        "            test_data = ep.tokenize_jiant_dataset(\n",
        "                tokenizer,\n",
        "                *(ep.read_jiant_dataset(f\"/content/data/{task}/{size}/test.jsonl\")),\n",
        "                labels_to_ids,\n",
        "                )\n",
        "        if task_types[task] == \"single_span\":\n",
        "            train_data = JiantDatasetSingleSpan(train_data)\n",
        "            val_data = JiantDatasetSingleSpan(val_data)\n",
        "            test_data = JiantDatasetSingleSpan(test_data)\n",
        "        elif task_types[task] == \"two_span\":\n",
        "            train_data = JiantDatasetTwoSpan(train_data)\n",
        "            val_data = JiantDatasetTwoSpan(val_data)\n",
        "            test_data = JiantDatasetTwoSpan(test_data)\n",
        "        train_loader = data.DataLoader(train_data, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=num_workers)\n",
        "        val_loader = data.DataLoader(val_data, batch_size=batch_size, pin_memory=True, num_workers=num_workers)\n",
        "        test_loader = data.DataLoader(test_data, batch_size=batch_size, pin_memory=True, num_workers=num_workers)\n",
        "        if tpu:\n",
        "            results[task][model] = ep_tpu.probing(ep.ProbeConfig(\n",
        "                train_loader,\n",
        "                val_loader,\n",
        "                test_loader,\n",
        "                model,\n",
        "                num_layers,\n",
        "                loss_function,\n",
        "                labels_to_ids,\n",
        "                task_types[task],\n",
        "                lr=0.0001,\n",
        "                patience=5,\n",
        "                eval_interval=1000,\n",
        "                dev=device,\n",
        "                results_path=\"/content/drive/MyDrive/replicated/intermediate-results/\",\n",
        "                ))\n",
        "        else:\n",
        "            results[task][model] = ep.probing(ep.ProbeConfig(\n",
        "                train_loader,\n",
        "                val_loader,\n",
        "                test_loader,\n",
        "                model,\n",
        "                num_layers,\n",
        "                loss_function,\n",
        "                labels_to_ids,\n",
        "                task_types[task],\n",
        "                lr=0.0001,\n",
        "                patience=5,\n",
        "                eval_interval=1000,\n",
        "                dev=device,\n",
        "                results_path=\"/content/drive/MyDrive/replicated/intermediate-results/\",\n",
        "                ))\n",
        "        with open(f'/content/drive/MyDrive/replicated/probing-results/{size}.json', 'w') as f:\n",
        "            json.dump(results, f)"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "zi8vPUx5L-e4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Visualize the results:**"
      ],
      "metadata": {
        "id": "1YTm3edA3tWQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "model_to_label = {\n",
        "    \"roberta-base\": \"RoBERTa\",\n",
        "    \"bert-base-uncased\": \"no fine-tuning\", \n",
        "    \"csarron/bert-base-uncased-squad-v1\": \"SQuAD\",\n",
        "    \"/content/drive/MyDrive/BERT_fineTunned_on_AdversarialQA_trained\": \"adversarialQA\",\n",
        "    }\n",
        "model_to_linestyle = {\n",
        "    \"bert-base-uncased\": \":g\", \n",
        "    \"csarron/bert-base-uncased-squad-v1\": \"-y\",\n",
        "    \"/content/drive/MyDrive/BERT_fineTunned_on_AdversarialQA_trained\": \"--b\",\n",
        "    \"roberta-base\": \"-.r\",\n",
        "    }\n",
        "\n",
        "task_to_title = {\n",
        "    \"coref\": \"COREF\",\n",
        "    \"ques\": \"Question Type\",\n",
        "    \"sup-squad\": \"Supporting Facts SQuAD\",\n",
        "    \"sup-babi\": \"Supporting Facts bAbI\",\n",
        "    \"ner\": \"Named Entity Labeling\",\n",
        "    \"semeval\": \"Relation Classification\",\n",
        "    \"adversarialqa\": \"Adversarial QA\"\n",
        "    }\n",
        "\n",
        "f, axs = plt.subplots(1, len(tasks), figsize=(len(tasks)*5 + 1, 3))\n",
        "i = 0\n",
        "\n",
        "for task in tasks:\n",
        "    for model in models:\n",
        "        ep.plot_task(\n",
        "            f\"f'/content/drive/MyDrive/replicated/probing-results/{size}.json\",\n",
        "            task,\n",
        "            model,\n",
        "            model_to_linestyle[model],\n",
        "            num_layers,\n",
        "            label=model_to_label[model],\n",
        "            title=task_to_title[task],\n",
        "            plot=axs[i]\n",
        "            )\n",
        "    i += 1\n",
        "plt.show()"
      ],
      "outputs": [],
      "metadata": {
        "id": "QBly9n4SL-e4"
      }
    }
  ]
}