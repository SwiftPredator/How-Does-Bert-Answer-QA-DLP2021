{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "preprocess-probing-data.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "metadata": {
      "interpreter": {
        "hash": "a76e1f3b3a43d56eebec3005daa29b5127721f42f968f9183bdc32f9f210ea51"
      }
    }
  },
  "cells": [
    {
      "source": [
        "**Use this notebook to get the ner, rel and coref datasets**"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "Install dependencies for jiant 1.3\n",
        "\n",
        "Install dependencies for nltk\n",
        "\n",
        "Install overrides==3.1.0, compare https://github.com/allenai/allennlp/issues/5217\n",
        "\n",
        "Install tensorflow==1.15"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "R4O43zb_xUR0"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gC82xIyaxV48",
        "outputId": "db44877b-04f1-49d8-8236-02a120681ef6"
      },
      "source": [
        "!pip install allennlp==0.8.4\n",
        "!pip install overrides==3.1.0\n",
        "!pip install jsondiff\n",
        "!pip install sacremoses\n",
        "!pip install pyhocon==0.3.35\n",
        "!pip install transformers==2.6.0\n",
        "!pip install python-Levenshtein==0.12.0\n",
        "!pip install tensorflow==1.15.0\n",
        "\n",
        "!python -m nltk.downloader perluniprops nonbreaking_prefixes punkt\n",
        "\n",
        "!pip uninstall overrides\n",
        "!pip install overrides==3.1.0\n",
        "\n",
        "!pip install tensorflow==1.15"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting allennlp==0.8.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/30/8c/72b14d20c9cbb0306939ea41109fc599302634fd5c59ccba1a659b7d0360/allennlp-0.8.4-py3-none-any.whl (5.7MB)\n",
            "\u001b[K     |████████████████████████████████| 5.7MB 7.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from allennlp==0.8.4) (1.9.0+cu102)\n",
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.7/dist-packages (from allennlp==0.8.4) (0.5.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from allennlp==0.8.4) (0.22.2.post1)\n",
            "Collecting spacy<2.2,>=2.0.18\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/16/f3/554271be8ff46471586d164bfbb6999364ba30ca5a0045e2a86da5f3b2c5/spacy-2.1.9-cp37-cp37m-manylinux1_x86_64.whl (30.8MB)\n",
            "\u001b[K     |████████████████████████████████| 30.8MB 99kB/s \n",
            "\u001b[?25hCollecting parsimonious>=0.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/fc/067a3f89869a41009e1a7cdfb14725f8ddd246f30f63c645e8ef8a1c56f4/parsimonious-0.8.1.tar.gz (45kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 7.0MB/s \n",
            "\u001b[?25hCollecting pytorch-pretrained-bert>=0.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 23.4MB/s \n",
            "\u001b[?25hCollecting word2number>=1.1\n",
            "  Downloading https://files.pythonhosted.org/packages/4a/29/a31940c848521f0725f0df6b25dca8917f13a2025b0e8fcbe5d0457e45e6/word2number-1.1.zip\n",
            "Requirement already satisfied: tqdm>=4.19 in /usr/local/lib/python3.7/dist-packages (from allennlp==0.8.4) (4.41.1)\n",
            "Collecting responses>=0.7\n",
            "  Downloading https://files.pythonhosted.org/packages/ba/00/0e63b7024c2d873bf57411ab0ed77eeafd5f44bace7cbf1d56bca8ab3be2/responses-0.13.3-py2.py3-none-any.whl\n",
            "Collecting ftfy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/af/da/d215a091986e5f01b80f5145cff6f22e2dc57c6b048aab2e882a07018473/ftfy-6.0.3.tar.gz (64kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 9.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (from allennlp==0.8.4) (3.6.4)\n",
            "Requirement already satisfied: sqlparse>=0.2.4 in /usr/local/lib/python3.7/dist-packages (from allennlp==0.8.4) (0.4.1)\n",
            "Collecting jsonnet>=0.10.0; sys_platform != \"win32\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/42/40/6f16e5ac994b16fa71c24310f97174ce07d3a97b433275589265c6b94d2b/jsonnet-0.17.0.tar.gz (259kB)\n",
            "\u001b[K     |████████████████████████████████| 266kB 42.1MB/s \n",
            "\u001b[?25hCollecting tensorboardX>=1.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/99/0b/a26bbe92667c549d39c40b80c5ddec638fbae9521f04aeef26560e07e504/tensorboardX-2.4-py2.py3-none-any.whl (124kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 44.7MB/s \n",
            "\u001b[?25hCollecting boto3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2c/e1/2c6c374f043c3f22829563b7fb2bf28fe3dca7ce5994bc5ceeff0959d6c9/boto3-1.17.105-py2.py3-none-any.whl (131kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 45.3MB/s \n",
            "\u001b[?25hCollecting awscli>=1.11.91\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/02/fd39da3e7290346461020efafc034096a8d58431eda1dbc3441785405fdc/awscli-1.19.105-py2.py3-none-any.whl (3.6MB)\n",
            "\u001b[K     |████████████████████████████████| 3.6MB 30.3MB/s \n",
            "\u001b[?25hCollecting unidecode\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/25/723487ca2a52ebcee88a34d7d1f5a4b80b793f179ee0f62d5371938dfa01/Unidecode-1.2.0-py2.py3-none-any.whl (241kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 45.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from allennlp==0.8.4) (3.2.5)\n",
            "Collecting gevent>=1.3.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3e/85/df3d1fd2b60a87455475f93012861b76a411d27ba4a0859939adbe2c9dc3/gevent-21.1.2-cp37-cp37m-manylinux2010_x86_64.whl (5.6MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6MB 21.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from allennlp==0.8.4) (2018.9)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from allennlp==0.8.4) (1.4.1)\n",
            "Collecting flaky\n",
            "  Downloading https://files.pythonhosted.org/packages/43/0e/2f50064e327f41a1eb811df089f813036e19a64b95e33f8e9e0b96c2447e/flaky-3.7.0-py2.py3-none-any.whl\n",
            "Collecting conllu==0.11\n",
            "  Downloading https://files.pythonhosted.org/packages/d4/2c/856344d9b69baf5b374c395b4286626181a80f0c2b2f704914d18a1cea47/conllu-0.11-py2.py3-none-any.whl\n",
            "Requirement already satisfied: requests>=2.18 in /usr/local/lib/python3.7/dist-packages (from allennlp==0.8.4) (2.23.0)\n",
            "Collecting overrides\n",
            "  Downloading https://files.pythonhosted.org/packages/db/52/1ee39040faa35ee9a609a4c7c95b017713f0b8d63399d5ecbaa6555a3cea/overrides-6.1.0-py3-none-any.whl\n",
            "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.7/dist-packages (from allennlp==0.8.4) (3.2.2)\n",
            "Collecting jsonpickle\n",
            "  Downloading https://files.pythonhosted.org/packages/bb/1a/f2db026d4d682303793559f1c2bb425ba3ec0d6fd7ac63397790443f2461/jsonpickle-2.0.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from allennlp==0.8.4) (1.19.5)\n",
            "Collecting numpydoc>=0.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/60/1d/9e398c53d6ae27d5ab312ddc16a9ffe1bee0dfdf1d6ec88c40b0ca97582e/numpydoc-1.1.0-py3-none-any.whl (47kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 7.3MB/s \n",
            "\u001b[?25hCollecting flask-cors>=3.0.7\n",
            "  Downloading https://files.pythonhosted.org/packages/db/84/901e700de86604b1c4ef4b57110d4e947c218b9997adf5d38fa7da493bce/Flask_Cors-3.0.10-py2.py3-none-any.whl\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from allennlp==0.8.4) (3.1.0)\n",
            "Requirement already satisfied: flask>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from allennlp==0.8.4) (1.1.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.1->allennlp==0.8.4) (3.7.4.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->allennlp==0.8.4) (1.0.1)\n",
            "Collecting blis<0.3.0,>=0.2.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fa/5f/47b7b29ad202b2210020e2f33bfb06d1db2abe0e709c2a84736e8a9d1bd5/blis-0.2.4-cp37-cp37m-manylinux1_x86_64.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 26.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<2.2,>=2.0.18->allennlp==0.8.4) (1.0.5)\n",
            "Collecting thinc<7.1.0,>=7.0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/42/d7ea7539af3852fd8c1f0b3adf4a100fb3d72b40b69cef1a764ff979a743/thinc-7.0.8-cp37-cp37m-manylinux1_x86_64.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 26.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<2.2,>=2.0.18->allennlp==0.8.4) (0.8.2)\n",
            "Collecting preshed<2.1.0,>=2.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/2b/3ecd5d90d2d6fd39fbc520de7d80db5d74defdc2d7c2e15531d9cc3498c7/preshed-2.0.1-cp37-cp37m-manylinux1_x86_64.whl (82kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 1.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: srsly<1.1.0,>=0.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<2.2,>=2.0.18->allennlp==0.8.4) (1.0.5)\n",
            "Collecting plac<1.0.0,>=0.9.6\n",
            "  Downloading https://files.pythonhosted.org/packages/9e/9b/62c60d2f5bc135d2aa1d8c8a86aaf84edb719a59c7f11a4316259e61a298/plac-0.9.6-py2.py3-none-any.whl\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<2.2,>=2.0.18->allennlp==0.8.4) (2.0.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from parsimonious>=0.8.0->allennlp==0.8.4) (1.15.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert>=0.6.0->allennlp==0.8.4) (2019.12.20)\n",
            "Collecting urllib3>=1.25.10\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5f/64/43575537846896abac0b15c3e5ac678d787a4021e906703f1766bfb8ea11/urllib3-1.26.6-py2.py3-none-any.whl (138kB)\n",
            "\u001b[K     |████████████████████████████████| 143kB 45.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy->allennlp==0.8.4) (0.2.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp==0.8.4) (57.0.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp==0.8.4) (21.2.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp==0.8.4) (0.7.1)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp==0.8.4) (1.4.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp==0.8.4) (8.8.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp==0.8.4) (1.10.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX>=1.2->allennlp==0.8.4) (3.12.4)\n",
            "Collecting botocore<1.21.0,>=1.20.105\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/95/da/3417300f85ba5173e8dba9248b9ae8bcb74a8aac1c92fa3d257f99073b9e/botocore-1.20.105-py2.py3-none-any.whl (7.7MB)\n",
            "\u001b[K     |████████████████████████████████| 7.7MB 24.7MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n",
            "Collecting s3transfer<0.5.0,>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/63/d0/693477c688348654ddc21dcdce0817653a294aa43f41771084c25e7ff9c7/s3transfer-0.4.2-py2.py3-none-any.whl (79kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 9.6MB/s \n",
            "\u001b[?25hCollecting colorama<0.4.4,>=0.2.5\n",
            "  Downloading https://files.pythonhosted.org/packages/c9/dc/45cdef1b4d119eb96316b3117e6d5708a08029992b2fee2c143c7a0a5cc5/colorama-0.4.3-py2.py3-none-any.whl\n",
            "Requirement already satisfied: rsa<4.8,>=3.1.2; python_version > \"2.7\" in /usr/local/lib/python3.7/dist-packages (from awscli>=1.11.91->allennlp==0.8.4) (4.7.2)\n",
            "Collecting docutils<0.16,>=0.10\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/22/cd/a6aa959dca619918ccb55023b4cb151949c64d4d5d55b3f4ffd7eee0c6e8/docutils-0.15.2-py3-none-any.whl (547kB)\n",
            "\u001b[K     |████████████████████████████████| 552kB 42.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML<5.5,>=3.10 in /usr/local/lib/python3.7/dist-packages (from awscli>=1.11.91->allennlp==0.8.4) (3.13)\n",
            "Collecting zope.event\n",
            "  Downloading https://files.pythonhosted.org/packages/9e/85/b45408c64f3b888976f1d5b37eed8d746b8d5729a66a49ec846fda27d371/zope.event-4.5.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: greenlet<2.0,>=0.4.17; platform_python_implementation == \"CPython\" in /usr/local/lib/python3.7/dist-packages (from gevent>=1.3.6->allennlp==0.8.4) (1.1.0)\n",
            "Collecting zope.interface\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/a7/94e1a92c71436f934cdd2102826fa041c83dcb7d21dd0f1fb1a57f6e0620/zope.interface-5.4.0-cp37-cp37m-manylinux2010_x86_64.whl (251kB)\n",
            "\u001b[K     |████████████████████████████████| 256kB 39.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp==0.8.4) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp==0.8.4) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp==0.8.4) (2021.5.30)\n",
            "Collecting typing-utils>=0.0.3\n",
            "  Downloading https://files.pythonhosted.org/packages/66/6e/99fe160a19676051070aa1da4ca44aac2a213e79ba1a492b5f03facf6447/typing_utils-0.1.0-py3-none-any.whl\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->allennlp==0.8.4) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->allennlp==0.8.4) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->allennlp==0.8.4) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->allennlp==0.8.4) (2.4.7)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from jsonpickle->allennlp==0.8.4) (4.5.0)\n",
            "Requirement already satisfied: Jinja2>=2.3 in /usr/local/lib/python3.7/dist-packages (from numpydoc>=0.8.0->allennlp==0.8.4) (2.11.3)\n",
            "Requirement already satisfied: sphinx>=1.6.5 in /usr/local/lib/python3.7/dist-packages (from numpydoc>=0.8.0->allennlp==0.8.4) (1.8.5)\n",
            "Requirement already satisfied: cached-property; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from h5py->allennlp==0.8.4) (1.5.2)\n",
            "Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.7/dist-packages (from flask>=1.0.2->allennlp==0.8.4) (1.0.1)\n",
            "Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.7/dist-packages (from flask>=1.0.2->allennlp==0.8.4) (7.1.2)\n",
            "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from flask>=1.0.2->allennlp==0.8.4) (1.1.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<4.8,>=3.1.2; python_version > \"2.7\"->awscli>=1.11.91->allennlp==0.8.4) (0.4.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->jsonpickle->allennlp==0.8.4) (3.4.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.3->numpydoc>=0.8.0->allennlp==0.8.4) (2.0.1)\n",
            "Requirement already satisfied: babel!=2.0,>=1.3 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp==0.8.4) (2.9.1)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp==0.8.4) (2.6.1)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp==0.8.4) (1.2.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp==0.8.4) (20.9)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp==0.8.4) (0.7.12)\n",
            "Requirement already satisfied: sphinxcontrib-websupport in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp==0.8.4) (1.2.4)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp==0.8.4) (2.1.0)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml in /usr/local/lib/python3.7/dist-packages (from sphinxcontrib-websupport->sphinx>=1.6.5->numpydoc>=0.8.0->allennlp==0.8.4) (1.1.5)\n",
            "Building wheels for collected packages: parsimonious, word2number, ftfy, jsonnet\n",
            "  Building wheel for parsimonious (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for parsimonious: filename=parsimonious-0.8.1-cp37-none-any.whl size=42726 sha256=b1b0e48f5ae23c616a7041fad578ccf5bc1d9fb3a77f5f01a61a885f341c1129\n",
            "  Stored in directory: /root/.cache/pip/wheels/b7/8d/e7/a0e74217da5caeb3c1c7689639b6d28ddbf9985b840bc96a9a\n",
            "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for word2number: filename=word2number-1.1-cp37-none-any.whl size=5584 sha256=a0acaa203a22fcb6ba46691cc64c37ea8fdebec9303d8b9ec5dd0a8032d69563\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/2f/53/5f5c1d275492f2fce1cdab9a9bb12d49286dead829a4078e0e\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ftfy: filename=ftfy-6.0.3-cp37-none-any.whl size=41935 sha256=b9c89acf92fceca51e0746519c61f56ee60934dcc1a6d64af9edee3944d75a89\n",
            "  Stored in directory: /root/.cache/pip/wheels/99/2c/e6/109c8a28fef7a443f67ba58df21fe1d0067ac3322e75e6b0b7\n",
            "  Building wheel for jsonnet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jsonnet: filename=jsonnet-0.17.0-cp37-cp37m-linux_x86_64.whl size=3388747 sha256=48d6a597da51be637aadc0737af3990cfbb70fedf6ed9d1243f97c67988203da\n",
            "  Stored in directory: /root/.cache/pip/wheels/26/7a/37/7dbcc30a6b4efd17b91ad1f0128b7bbf84813bd4e1cfb8c1e3\n",
            "Successfully built parsimonious word2number ftfy jsonnet\n",
            "\u001b[31mERROR: requests 2.23.0 has requirement urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1, but you'll have urllib3 1.26.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: en-core-web-sm 2.2.5 has requirement spacy>=2.2.2, but you'll have spacy 2.1.9 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: blis, preshed, plac, thinc, spacy, parsimonious, jmespath, urllib3, botocore, s3transfer, boto3, pytorch-pretrained-bert, word2number, responses, ftfy, jsonnet, tensorboardX, colorama, docutils, awscli, unidecode, zope.event, zope.interface, gevent, flaky, conllu, typing-utils, overrides, jsonpickle, numpydoc, flask-cors, allennlp\n",
            "  Found existing installation: blis 0.4.1\n",
            "    Uninstalling blis-0.4.1:\n",
            "      Successfully uninstalled blis-0.4.1\n",
            "  Found existing installation: preshed 3.0.5\n",
            "    Uninstalling preshed-3.0.5:\n",
            "      Successfully uninstalled preshed-3.0.5\n",
            "  Found existing installation: plac 1.1.3\n",
            "    Uninstalling plac-1.1.3:\n",
            "      Successfully uninstalled plac-1.1.3\n",
            "  Found existing installation: thinc 7.4.0\n",
            "    Uninstalling thinc-7.4.0:\n",
            "      Successfully uninstalled thinc-7.4.0\n",
            "  Found existing installation: spacy 2.2.4\n",
            "    Uninstalling spacy-2.2.4:\n",
            "      Successfully uninstalled spacy-2.2.4\n",
            "  Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Found existing installation: docutils 0.17.1\n",
            "    Uninstalling docutils-0.17.1:\n",
            "      Successfully uninstalled docutils-0.17.1\n",
            "Successfully installed allennlp-0.8.4 awscli-1.19.105 blis-0.2.4 boto3-1.17.105 botocore-1.20.105 colorama-0.4.3 conllu-0.11 docutils-0.15.2 flaky-3.7.0 flask-cors-3.0.10 ftfy-6.0.3 gevent-21.1.2 jmespath-0.10.0 jsonnet-0.17.0 jsonpickle-2.0.0 numpydoc-1.1.0 overrides-6.1.0 parsimonious-0.8.1 plac-0.9.6 preshed-2.0.1 pytorch-pretrained-bert-0.6.2 responses-0.13.3 s3transfer-0.4.2 spacy-2.1.9 tensorboardX-2.4 thinc-7.0.8 typing-utils-0.1.0 unidecode-1.2.0 urllib3-1.26.6 word2number-1.1 zope.event-4.5.0 zope.interface-5.4.0\n",
            "Collecting overrides==3.1.0\n",
            "  Downloading https://files.pythonhosted.org/packages/ff/b1/10f69c00947518e6676bbd43e739733048de64b8dd998e9c2d5a71f44c5d/overrides-3.1.0.tar.gz\n",
            "Building wheels for collected packages: overrides\n",
            "  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for overrides: filename=overrides-3.1.0-cp37-none-any.whl size=10187 sha256=f52c2d75f0cd638246a6f2bf9b1e9095becc683b31a8665c2ecf59934236f18c\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/24/13/6ef8600e6f147c95e595f1289a86a3cc82ed65df57582c65a9\n",
            "Successfully built overrides\n",
            "Installing collected packages: overrides\n",
            "  Found existing installation: overrides 6.1.0\n",
            "    Uninstalling overrides-6.1.0:\n",
            "      Successfully uninstalled overrides-6.1.0\n",
            "Successfully installed overrides-3.1.0\n",
            "Collecting jsondiff\n",
            "  Downloading https://files.pythonhosted.org/packages/37/53/df2401ddcdc20289e715d3ab30080a0f286a897b6c9c6511bad869ee1ea1/jsondiff-1.3.0.tar.gz\n",
            "Building wheels for collected packages: jsondiff\n",
            "  Building wheel for jsondiff (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jsondiff: filename=jsondiff-1.3.0-cp37-none-any.whl size=6588 sha256=e60c086b0ee3b3fc03821e34e54c6f8194081cd2b7442ab53e04e19c2ba94157\n",
            "  Stored in directory: /root/.cache/pip/wheels/a8/20/b5/c3599df1109bfbea3ce3aa5fe11862d6841e933774b4013ce5\n",
            "Successfully built jsondiff\n",
            "Installing collected packages: jsondiff\n",
            "Successfully installed jsondiff-1.3.0\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 8.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from sacremoses) (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sacremoses) (4.41.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses) (1.0.1)\n",
            "Installing collected packages: sacremoses\n",
            "Successfully installed sacremoses-0.0.45\n",
            "Collecting pyhocon==0.3.35\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/95/b9/72883593ce531e95ac8190e251ae6f5377eada69504248bf1aebfce4c5b4/pyhocon-0.3.35.tar.gz (94kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 4.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing>=2.0.3 in /usr/local/lib/python3.7/dist-packages (from pyhocon==0.3.35) (2.4.7)\n",
            "Building wheels for collected packages: pyhocon\n",
            "  Building wheel for pyhocon (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyhocon: filename=pyhocon-0.3.35-cp37-none-any.whl size=14124 sha256=6e44709e6080a6e15736c06c42afa76df5829a762df1fdfa66b6c7fd79b1363a\n",
            "  Stored in directory: /root/.cache/pip/wheels/97/ea/cd/d2b70c1cfa9e6b1242709de5a64fd46be8982bc42be56d9390\n",
            "Successfully built pyhocon\n",
            "Installing collected packages: pyhocon\n",
            "Successfully installed pyhocon-0.3.35\n",
            "Collecting transformers==2.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/a0/32e3a4501ef480f7ea01aac329a716132f32f7911ef1c2fac228acc57ca7/transformers-2.6.0-py3-none-any.whl (540kB)\n",
            "\u001b[K     |████████████████████████████████| 542kB 6.9MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.5.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d6/e3/5e49e9a83fb605aaa34a1c1173e607302fecae529428c28696fb18f1c2c9/tokenizers-0.5.2-cp37-cp37m-manylinux1_x86_64.whl (5.6MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6MB 12.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==2.6.0) (4.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==2.6.0) (3.0.12)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ac/aa/1437691b0c7c83086ebb79ce2da16e00bef024f24fec2a5161c35476f499/sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 37.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.7/dist-packages (from transformers==2.6.0) (1.17.105)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==2.6.0) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==2.6.0) (1.19.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==2.6.0) (2019.12.20)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==2.6.0) (0.0.45)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3->transformers==2.6.0) (0.10.0)\n",
            "Requirement already satisfied: botocore<1.21.0,>=1.20.105 in /usr/local/lib/python3.7/dist-packages (from boto3->transformers==2.6.0) (1.20.105)\n",
            "Requirement already satisfied: s3transfer<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from boto3->transformers==2.6.0) (0.4.2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.6.0) (2.10)\n",
            "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/56/aa/4ef5aa67a9a62505db124a5cb5262332d1d4153462eb8fd89c9fa41e5d92/urllib3-1.25.11-py2.py3-none-any.whl (127kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 33.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.6.0) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.6.0) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.6.0) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.6.0) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.6.0) (1.0.1)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.21.0,>=1.20.105->boto3->transformers==2.6.0) (2.8.1)\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tokenizers, sentencepiece, transformers, urllib3\n",
            "  Found existing installation: urllib3 1.26.6\n",
            "    Uninstalling urllib3-1.26.6:\n",
            "      Successfully uninstalled urllib3-1.26.6\n",
            "Successfully installed sentencepiece-0.1.96 tokenizers-0.5.2 transformers-2.6.0 urllib3-1.25.11\n",
            "\u001b[33mWARNING: The candidate selected for download or install is a yanked version: 'python-Levenshtein' candidate (version 0.12.0 at https://files.pythonhosted.org/packages/42/a9/d1785c85ebf9b7dfacd08938dd028209c34a0ea3b1bcdb895208bd40a67d/python-Levenshtein-0.12.0.tar.gz#sha256=033a11de5e3d19ea25c9302d11224e1a1898fe5abd23c61c7c360c25195e3eb1 (from https://pypi.org/simple/python-levenshtein/))\n",
            "Reason for being yanked: Insecure, upgrade to 0.12.1\u001b[0m\n",
            "Collecting python-Levenshtein==0.12.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/42/a9/d1785c85ebf9b7dfacd08938dd028209c34a0ea3b1bcdb895208bd40a67d/python-Levenshtein-0.12.0.tar.gz (48kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 3.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from python-Levenshtein==0.12.0) (57.0.0)\n",
            "Building wheels for collected packages: python-Levenshtein\n",
            "  Building wheel for python-Levenshtein (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-Levenshtein: filename=python_Levenshtein-0.12.0-cp37-cp37m-linux_x86_64.whl size=145879 sha256=e4b31aea3ece2c66eee24ed3d6c4f3e84758675554d6864a1a13546b9c03eb04\n",
            "  Stored in directory: /root/.cache/pip/wheels/de/c2/93/660fd5f7559049268ad2dc6d81c4e39e9e36518766eaf7e342\n",
            "Successfully built python-Levenshtein\n",
            "Installing collected packages: python-Levenshtein\n",
            "Successfully installed python-Levenshtein-0.12.0\n",
            "Collecting tensorflow==1.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/92/2b/e3af15221da9ff323521565fa3324b0d7c7c5b1d7a8ca66984c8d59cb0ce/tensorflow-1.15.0-cp37-cp37m-manylinux2010_x86_64.whl (412.3MB)\n",
            "\u001b[K     |████████████████████████████████| 412.3MB 38kB/s \n",
            "\u001b[?25hRequirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (0.2.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (0.36.2)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (3.12.4)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.12.1)\n",
            "Collecting keras-applications>=1.0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 3.5MB/s \n",
            "\u001b[?25hCollecting tensorboard<1.16.0,>=1.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/e9/d3d747a97f7188f48aa5eda486907f3b345cd409f0a0850468ba867db246/tensorboard-1.15.0-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 32.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.19.5)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (0.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.1.0)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.15.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (3.3.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (0.8.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.34.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.1.2)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/62/2ee9cd74c9fa2fa450877847ba560b260f5d0fb70ee0595203082dafcc9d/tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503kB)\n",
            "\u001b[K     |████████████████████████████████| 512kB 42.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.6.1->tensorflow==1.15.0) (57.0.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15.0) (3.1.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (3.3.4)\n",
            "Requirement already satisfied: cached-property; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.8->tensorflow==1.15.0) (1.5.2)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (4.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (3.4.1)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp37-none-any.whl size=7557 sha256=48551ab3553c375ef089b5efe25a3bde7ba5cc4099903b076a172f2faec11616\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "Successfully built gast\n",
            "\u001b[31mERROR: tensorflow-probability 0.12.1 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: kapre 0.3.5 has requirement tensorflow>=2.0.0, but you'll have tensorflow 1.15.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: keras-applications, tensorboard, gast, tensorflow-estimator, tensorflow\n",
            "  Found existing installation: tensorboard 2.5.0\n",
            "    Uninstalling tensorboard-2.5.0:\n",
            "      Successfully uninstalled tensorboard-2.5.0\n",
            "  Found existing installation: gast 0.4.0\n",
            "    Uninstalling gast-0.4.0:\n",
            "      Successfully uninstalled gast-0.4.0\n",
            "  Found existing installation: tensorflow-estimator 2.5.0\n",
            "    Uninstalling tensorflow-estimator-2.5.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.5.0\n",
            "  Found existing installation: tensorflow 2.5.0\n",
            "    Uninstalling tensorflow-2.5.0:\n",
            "      Successfully uninstalled tensorflow-2.5.0\n",
            "Successfully installed gast-0.2.2 keras-applications-1.0.8 tensorboard-1.15.0 tensorflow-1.15.0 tensorflow-estimator-1.15.1\n",
            "/usr/lib/python3.7/runpy.py:125: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
            "  warn(RuntimeWarning(msg))\n",
            "[nltk_data] Downloading package perluniprops to /root/nltk_data...\n",
            "[nltk_data]   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data] Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "Uninstalling overrides-3.1.0:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.7/dist-packages/overrides-3.1.0.dist-info/*\n",
            "    /usr/local/lib/python3.7/dist-packages/overrides/*\n",
            "Proceed (y/n)? "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmxEMgvsf7GE"
      },
      "source": [
        "clone jiant and OntoNotes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bPpCTvbAf538",
        "outputId": "88288524-b607-433c-ffe3-b99c3bd3f183"
      },
      "source": [
        "!git clone --branch v1.3.2  --recursive https://github.com/nyu-mll/jiant.git jiant\n",
        "!git clone https://github.com/yuchenlin/OntoNotes-5.0-NER-BIO.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'jiant' already exists and is not an empty directory.\n",
            "fatal: destination path 'OntoNotes-5.0-NER-BIO' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OB8RnpFA8Zs8"
      },
      "source": [
        "set a few environment variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OeTyhpGr1yGA"
      },
      "source": [
        "import os\n",
        "os.environ['JIANT_PROJECT_PREFIX'] = 'output'\n",
        "os.environ['JIANT_DATA_DIR'] = 'data'\n",
        "os.environ['WORD_EMBS_FILE'] = 'embs'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvpD_1Hr8zdY"
      },
      "source": [
        "Copy the ontonotes path (/content/OntoNotes-5.0-NER-BIO/conll-formatted-ontonotes-5.0) to /content/jiant/probing/get_and_process_all_data.sh\n",
        "\n",
        "Comment out SPR data and tokenizing for OpenAI, Moses and bert-large in /content/jiant/probing/get_and_process_all_data.sh. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cOe5JKh_9Vlw",
        "outputId": "78413566-595a-4762-adba-011c799c29f4"
      },
      "source": [
        "!./jiant/probing/get_and_process_all_data.sh"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+ OUTPUT_DIR=/root/glue_data/edges\n",
            "+ mkdir -p /root/glue_data/edges\n",
            "++ dirname ./jiant/probing/get_and_process_all_data.sh\n",
            "+ HERE=./jiant/probing\n",
            "+ get_ontonotes\n",
            "+ python ./jiant/probing/data/extract_ontonotes_all.py --ontonotes /content/OntoNotes-5.0-NER-BIO/conll-formatted-ontonotes-5.0 --tasks const coref ner srl --splits train development test conll-2012-test -o /root/glue_data/edges/ontonotes\n",
            "07/03 02:47:54 PM: Processing split 'train' for task 'const'\n",
            "0it [00:00, ?it/s]07/03 02:47:54 PM: Reading CONLL sentences from dataset files at: /content/OntoNotes-5.0-NER-BIO/conll-formatted-ontonotes-5.0/data/train\n",
            "115812it [02:57, 651.06it/s]\n",
            "07/03 02:50:52 PM: Wrote examples to /root/glue_data/edges/ontonotes/const/train.json\n",
            "07/03 02:50:52 PM: Stats:\n",
            "count                        115812.00\n",
            "token.count                 2200865.00\n",
            "token.mean_count                 19.00\n",
            "token.rms_count                  23.09\n",
            "token.max_count                 228.00\n",
            "targets.count               3921972.00\n",
            "targets.mean_count               33.86\n",
            "targets.max_count               360.00\n",
            "targets.label.count         3921972.00\n",
            "targets.label.mean_count          1.00\n",
            "targets.span1.mean_length         4.00\n",
            "targets.span2.mean_length         0.00\n",
            "dtype: float64\n",
            "\n",
            "07/03 02:50:52 PM: sentences       115812\n",
            "missing_tree      5298\n",
            "dtype: object\n",
            "07/03 02:50:52 PM: Processing split 'train' for task 'coref'\n",
            "0it [00:00, ?it/s]07/03 02:50:52 PM: Reading CONLL sentences from dataset files at: /content/OntoNotes-5.0-NER-BIO/conll-formatted-ontonotes-5.0/data/train\n",
            "115812it [00:53, 2149.64it/s]\n",
            "07/03 02:51:46 PM: Wrote examples to /root/glue_data/edges/ontonotes/coref/train.json\n",
            "07/03 02:51:46 PM: Stats:\n",
            "count                        115812.00\n",
            "token.count                 2200865.00\n",
            "token.mean_count                 19.00\n",
            "token.rms_count                  23.09\n",
            "token.max_count                 228.00\n",
            "targets.count                207830.00\n",
            "targets.mean_count                1.79\n",
            "targets.max_count               300.00\n",
            "targets.label.count          207830.00\n",
            "targets.label.mean_count          1.00\n",
            "targets.span1.mean_length         2.43\n",
            "targets.span2.mean_length         2.43\n",
            "dtype: float64\n",
            "\n",
            "07/03 02:51:46 PM: num_entities    155559\n",
            "sentences       115812\n",
            "dtype: object\n",
            "07/03 02:51:46 PM: Processing split 'train' for task 'ner'\n",
            "0it [00:00, ?it/s]07/03 02:51:46 PM: Reading CONLL sentences from dataset files at: /content/OntoNotes-5.0-NER-BIO/conll-formatted-ontonotes-5.0/data/train\n",
            "115812it [00:53, 2174.89it/s]\n",
            "07/03 02:52:39 PM: Wrote examples to /root/glue_data/edges/ontonotes/ner/train.json\n",
            "07/03 02:52:39 PM: Stats:\n",
            "count                        115812.00\n",
            "token.count                 2200865.00\n",
            "token.mean_count                 19.00\n",
            "token.rms_count                  23.09\n",
            "token.max_count                 228.00\n",
            "targets.count                128738.00\n",
            "targets.mean_count                1.11\n",
            "targets.max_count                32.00\n",
            "targets.label.count          128738.00\n",
            "targets.label.mean_count          1.00\n",
            "targets.span1.mean_length         1.86\n",
            "targets.span2.mean_length         0.00\n",
            "dtype: float64\n",
            "\n",
            "07/03 02:52:39 PM: sentences    115812\n",
            "dtype: object\n",
            "07/03 02:52:39 PM: Processing split 'train' for task 'srl'\n",
            "0it [00:00, ?it/s]07/03 02:52:39 PM: Reading CONLL sentences from dataset files at: /content/OntoNotes-5.0-NER-BIO/conll-formatted-ontonotes-5.0/data/train\n",
            "115812it [01:00, 1928.11it/s]\n",
            "07/03 02:53:39 PM: Wrote examples to /root/glue_data/edges/ontonotes/srl/train.json\n",
            "07/03 02:53:39 PM: Stats:\n",
            "count                        253070.00\n",
            "token.count                 6619740.00\n",
            "token.mean_count                 26.16\n",
            "token.rms_count                  30.50\n",
            "token.max_count                 210.00\n",
            "targets.count                598983.00\n",
            "targets.mean_count                2.37\n",
            "targets.max_count                11.00\n",
            "targets.label.count          598983.00\n",
            "targets.label.mean_count          1.00\n",
            "targets.span1.mean_length         1.00\n",
            "targets.span2.mean_length         4.27\n",
            "dtype: float64\n",
            "\n",
            "07/03 02:53:39 PM: frames       253070\n",
            "sentences    115812\n",
            "dtype: object\n",
            "07/03 02:53:39 PM: Processing split 'development' for task 'const'\n",
            "0it [00:00, ?it/s]07/03 02:53:39 PM: Reading CONLL sentences from dataset files at: /content/OntoNotes-5.0-NER-BIO/conll-formatted-ontonotes-5.0/data/development\n",
            "15680it [00:26, 601.61it/s]\n",
            "07/03 02:54:06 PM: Wrote examples to /root/glue_data/edges/ontonotes/const/development.json\n",
            "07/03 02:54:06 PM: Stats:\n",
            "count                        15680.00\n",
            "token.count                 304701.00\n",
            "token.mean_count                19.43\n",
            "token.rms_count                 23.55\n",
            "token.max_count                275.00\n",
            "targets.count               545146.00\n",
            "targets.mean_count              34.77\n",
            "targets.max_count              450.00\n",
            "targets.label.count         545146.00\n",
            "targets.label.mean_count         1.00\n",
            "targets.span1.mean_length        4.03\n",
            "targets.span2.mean_length        0.00\n",
            "dtype: float64\n",
            "\n",
            "07/03 02:54:06 PM: sentences       15680\n",
            "missing_tree      620\n",
            "dtype: object\n",
            "07/03 02:54:06 PM: Processing split 'development' for task 'coref'\n",
            "0it [00:00, ?it/s]07/03 02:54:06 PM: Reading CONLL sentences from dataset files at: /content/OntoNotes-5.0-NER-BIO/conll-formatted-ontonotes-5.0/data/development\n",
            "15680it [00:07, 2120.52it/s]\n",
            "07/03 02:54:13 PM: Wrote examples to /root/glue_data/edges/ontonotes/coref/development.json\n",
            "07/03 02:54:13 PM: Stats:\n",
            "count                        15680.00\n",
            "token.count                 304701.00\n",
            "token.mean_count                19.43\n",
            "token.rms_count                 23.55\n",
            "token.max_count                275.00\n",
            "targets.count                26333.00\n",
            "targets.mean_count               1.68\n",
            "targets.max_count              171.00\n",
            "targets.label.count          26333.00\n",
            "targets.label.mean_count         1.00\n",
            "targets.span1.mean_length        2.58\n",
            "targets.span2.mean_length        2.52\n",
            "dtype: float64\n",
            "\n",
            "07/03 02:54:13 PM: num_entities    19156\n",
            "sentences       15680\n",
            "dtype: object\n",
            "07/03 02:54:13 PM: Processing split 'development' for task 'ner'\n",
            "0it [00:00, ?it/s]07/03 02:54:13 PM: Reading CONLL sentences from dataset files at: /content/OntoNotes-5.0-NER-BIO/conll-formatted-ontonotes-5.0/data/development\n",
            "15680it [00:07, 2128.68it/s]\n",
            "07/03 02:54:20 PM: Wrote examples to /root/glue_data/edges/ontonotes/ner/development.json\n",
            "07/03 02:54:20 PM: Stats:\n",
            "count                        15680.00\n",
            "token.count                 304701.00\n",
            "token.mean_count                19.43\n",
            "token.rms_count                 23.55\n",
            "token.max_count                275.00\n",
            "targets.count                20354.00\n",
            "targets.mean_count               1.30\n",
            "targets.max_count               71.00\n",
            "targets.label.count          20354.00\n",
            "targets.label.mean_count         1.00\n",
            "targets.span1.mean_length        1.84\n",
            "targets.span2.mean_length        0.00\n",
            "dtype: float64\n",
            "\n",
            "07/03 02:54:20 PM: sentences    15680\n",
            "dtype: object\n",
            "07/03 02:54:20 PM: Processing split 'development' for task 'srl'\n",
            "0it [00:00, ?it/s]07/03 02:54:20 PM: Reading CONLL sentences from dataset files at: /content/OntoNotes-5.0-NER-BIO/conll-formatted-ontonotes-5.0/data/development\n",
            "15680it [00:08, 1869.56it/s]\n",
            "07/03 02:54:29 PM: Wrote examples to /root/glue_data/edges/ontonotes/srl/development.json\n",
            "07/03 02:54:29 PM: Stats:\n",
            "count                        35297.00\n",
            "token.count                 934744.00\n",
            "token.mean_count                26.48\n",
            "token.rms_count                 30.71\n",
            "token.max_count                275.00\n",
            "targets.count                83362.00\n",
            "targets.mean_count               2.36\n",
            "targets.max_count               10.00\n",
            "targets.label.count          83362.00\n",
            "targets.label.mean_count         1.00\n",
            "targets.span1.mean_length        1.00\n",
            "targets.span2.mean_length        4.41\n",
            "dtype: float64\n",
            "\n",
            "07/03 02:54:29 PM: frames       35297\n",
            "sentences    15680\n",
            "dtype: object\n",
            "07/03 02:54:29 PM: Processing split 'test' for task 'const'\n",
            "0it [00:00, ?it/s]07/03 02:54:29 PM: Reading CONLL sentences from dataset files at: /content/OntoNotes-5.0-NER-BIO/conll-formatted-ontonotes-5.0/data/test\n",
            "12217it [00:18, 659.88it/s]\n",
            "07/03 02:54:47 PM: Wrote examples to /root/glue_data/edges/ontonotes/const/test.json\n",
            "07/03 02:54:47 PM: Stats:\n",
            "count                        12217.00\n",
            "token.count                 230118.00\n",
            "token.mean_count                18.84\n",
            "token.rms_count                 23.08\n",
            "token.max_count                151.00\n",
            "targets.count               402656.00\n",
            "targets.mean_count              32.96\n",
            "targets.max_count              271.00\n",
            "targets.label.count         402656.00\n",
            "targets.label.mean_count         1.00\n",
            "targets.span1.mean_length        4.00\n",
            "targets.span2.mean_length        0.00\n",
            "dtype: float64\n",
            "\n",
            "07/03 02:54:47 PM: sentences       12217\n",
            "missing_tree      755\n",
            "dtype: object\n",
            "07/03 02:54:47 PM: Processing split 'test' for task 'coref'\n",
            "0it [00:00, ?it/s]07/03 02:54:47 PM: Reading CONLL sentences from dataset files at: /content/OntoNotes-5.0-NER-BIO/conll-formatted-ontonotes-5.0/data/test\n",
            "12217it [00:05, 2119.29it/s]\n",
            "07/03 02:54:53 PM: Wrote examples to /root/glue_data/edges/ontonotes/coref/test.json\n",
            "07/03 02:54:53 PM: Stats:\n",
            "count                        12217.00\n",
            "token.count                 230118.00\n",
            "token.mean_count                18.84\n",
            "token.rms_count                 23.08\n",
            "token.max_count                151.00\n",
            "targets.count                27800.00\n",
            "targets.mean_count               2.28\n",
            "targets.max_count              153.00\n",
            "targets.label.count          27800.00\n",
            "targets.label.mean_count         1.00\n",
            "targets.span1.mean_length        2.37\n",
            "targets.span2.mean_length        2.40\n",
            "dtype: float64\n",
            "\n",
            "07/03 02:54:53 PM: num_entities    19764\n",
            "sentences       12217\n",
            "dtype: object\n",
            "07/03 02:54:53 PM: Processing split 'test' for task 'ner'\n",
            "0it [00:00, ?it/s]07/03 02:54:53 PM: Reading CONLL sentences from dataset files at: /content/OntoNotes-5.0-NER-BIO/conll-formatted-ontonotes-5.0/data/test\n",
            "12217it [00:05, 2216.89it/s]\n",
            "07/03 02:54:59 PM: Wrote examples to /root/glue_data/edges/ontonotes/ner/test.json\n",
            "07/03 02:54:59 PM: Stats:\n",
            "count                        12217.00\n",
            "token.count                 230118.00\n",
            "token.mean_count                18.84\n",
            "token.rms_count                 23.08\n",
            "token.max_count                151.00\n",
            "targets.count                12586.00\n",
            "targets.mean_count               1.03\n",
            "targets.max_count               21.00\n",
            "targets.label.count          12586.00\n",
            "targets.label.mean_count         1.00\n",
            "targets.span1.mean_length        1.85\n",
            "targets.span2.mean_length        0.00\n",
            "dtype: float64\n",
            "\n",
            "07/03 02:54:59 PM: sentences    12217\n",
            "dtype: object\n",
            "07/03 02:54:59 PM: Processing split 'test' for task 'srl'\n",
            "0it [00:00, ?it/s]07/03 02:54:59 PM: Reading CONLL sentences from dataset files at: /content/OntoNotes-5.0-NER-BIO/conll-formatted-ontonotes-5.0/data/test\n",
            "12217it [00:06, 1919.45it/s]\n",
            "07/03 02:55:05 PM: Wrote examples to /root/glue_data/edges/ontonotes/srl/test.json\n",
            "07/03 02:55:05 PM: Stats:\n",
            "count                        26715.00\n",
            "token.count                 711746.00\n",
            "token.mean_count                26.64\n",
            "token.rms_count                 31.28\n",
            "token.max_count                151.00\n",
            "targets.count                61716.00\n",
            "targets.mean_count               2.31\n",
            "targets.max_count               11.00\n",
            "targets.label.count          61716.00\n",
            "targets.label.mean_count         1.00\n",
            "targets.span1.mean_length        1.00\n",
            "targets.span2.mean_length        4.20\n",
            "dtype: float64\n",
            "\n",
            "07/03 02:55:05 PM: sentences    12217\n",
            "frames       26715\n",
            "dtype: object\n",
            "07/03 02:55:05 PM: Processing split 'conll-2012-test' for task 'const'\n",
            "0it [00:00, ?it/s]07/03 02:55:05 PM: Reading CONLL sentences from dataset files at: /content/OntoNotes-5.0-NER-BIO/conll-formatted-ontonotes-5.0/data/conll-2012-test\n",
            "9479it [00:14, 665.70it/s]\n",
            "07/03 02:55:19 PM: Wrote examples to /root/glue_data/edges/ontonotes/const/conll-2012-test.json\n",
            "07/03 02:55:19 PM: Stats:\n",
            "count                         9479.00\n",
            "token.count                 169579.00\n",
            "token.mean_count                17.89\n",
            "token.rms_count                 22.15\n",
            "token.max_count                151.00\n",
            "targets.count               321689.00\n",
            "targets.mean_count              33.94\n",
            "targets.max_count              271.00\n",
            "targets.label.count         321689.00\n",
            "targets.label.mean_count         1.00\n",
            "targets.span1.mean_length        3.96\n",
            "targets.span2.mean_length        0.00\n",
            "dtype: float64\n",
            "\n",
            "07/03 02:55:19 PM: sentences       9479\n",
            "missing_tree      30\n",
            "dtype: object\n",
            "07/03 02:55:19 PM: Processing split 'conll-2012-test' for task 'coref'\n",
            "0it [00:00, ?it/s]07/03 02:55:19 PM: Reading CONLL sentences from dataset files at: /content/OntoNotes-5.0-NER-BIO/conll-formatted-ontonotes-5.0/data/conll-2012-test\n",
            "9479it [00:04, 2103.65it/s]\n",
            "07/03 02:55:24 PM: Wrote examples to /root/glue_data/edges/ontonotes/coref/conll-2012-test.json\n",
            "07/03 02:55:24 PM: Stats:\n",
            "count                         9479.00\n",
            "token.count                 169579.00\n",
            "token.mean_count                17.89\n",
            "token.rms_count                 22.15\n",
            "token.max_count                151.00\n",
            "targets.count                27800.00\n",
            "targets.mean_count               2.93\n",
            "targets.max_count              153.00\n",
            "targets.label.count          27800.00\n",
            "targets.label.mean_count         1.00\n",
            "targets.span1.mean_length        2.37\n",
            "targets.span2.mean_length        2.40\n",
            "dtype: float64\n",
            "\n",
            "07/03 02:55:24 PM: num_entities    19764\n",
            "sentences        9479\n",
            "dtype: object\n",
            "07/03 02:55:24 PM: Processing split 'conll-2012-test' for task 'ner'\n",
            "0it [00:00, ?it/s]07/03 02:55:24 PM: Reading CONLL sentences from dataset files at: /content/OntoNotes-5.0-NER-BIO/conll-formatted-ontonotes-5.0/data/conll-2012-test\n",
            "9479it [00:04, 2138.36it/s]\n",
            "07/03 02:55:28 PM: Wrote examples to /root/glue_data/edges/ontonotes/ner/conll-2012-test.json\n",
            "07/03 02:55:28 PM: Stats:\n",
            "count                         9479.00\n",
            "token.count                 169579.00\n",
            "token.mean_count                17.89\n",
            "token.rms_count                 22.15\n",
            "token.max_count                151.00\n",
            "targets.count                11257.00\n",
            "targets.mean_count               1.19\n",
            "targets.max_count               21.00\n",
            "targets.label.count          11257.00\n",
            "targets.label.mean_count         1.00\n",
            "targets.span1.mean_length        1.86\n",
            "targets.span2.mean_length        0.00\n",
            "dtype: float64\n",
            "\n",
            "07/03 02:55:28 PM: sentences    9479\n",
            "dtype: object\n",
            "07/03 02:55:28 PM: Processing split 'conll-2012-test' for task 'srl'\n",
            "0it [00:00, ?it/s]07/03 02:55:28 PM: Reading CONLL sentences from dataset files at: /content/OntoNotes-5.0-NER-BIO/conll-formatted-ontonotes-5.0/data/conll-2012-test\n",
            "9479it [00:05, 1809.95it/s]\n",
            "07/03 02:55:33 PM: Wrote examples to /root/glue_data/edges/ontonotes/srl/conll-2012-test.json\n",
            "07/03 02:55:33 PM: Stats:\n",
            "count                        24462.00\n",
            "token.count                 639622.00\n",
            "token.mean_count                26.15\n",
            "token.rms_count                 30.77\n",
            "token.max_count                151.00\n",
            "targets.count                55780.00\n",
            "targets.mean_count               2.28\n",
            "targets.max_count               10.00\n",
            "targets.label.count          55780.00\n",
            "targets.label.mean_count         1.00\n",
            "targets.span1.mean_length        1.00\n",
            "targets.span2.mean_length        4.10\n",
            "dtype: float64\n",
            "\n",
            "07/03 02:55:33 PM: sentences     9479\n",
            "frames       24462\n",
            "dtype: object\n",
            "+ python ./jiant/probing/split_constituent_data.py /root/glue_data/edges/ontonotes/const/conll-2012-test.json /root/glue_data/edges/ontonotes/const/development.json /root/glue_data/edges/ontonotes/const/test.json /root/glue_data/edges/ontonotes/const/train.json\n",
            "07/03 02:55:36 PM: Processing file: /root/glue_data/edges/ontonotes/const/conll-2012-test.json\n",
            "07/03 02:55:39 PM:   saving to /root/glue_data/edges/ontonotes/const/pos/conll-2012-test.json and /root/glue_data/edges/ontonotes/const/nonterminal/conll-2012-test.json\n",
            "100% 9479/9479 [00:13<00:00, 720.99it/s]\n",
            "07/03 02:55:52 PM: Processing file: /root/glue_data/edges/ontonotes/const/development.json\n",
            "07/03 02:55:58 PM:   saving to /root/glue_data/edges/ontonotes/const/pos/development.json and /root/glue_data/edges/ontonotes/const/nonterminal/development.json\n",
            "100% 15680/15680 [00:21<00:00, 745.99it/s]\n",
            "07/03 02:56:19 PM: Processing file: /root/glue_data/edges/ontonotes/const/test.json\n",
            "07/03 02:56:23 PM:   saving to /root/glue_data/edges/ontonotes/const/pos/test.json and /root/glue_data/edges/ontonotes/const/nonterminal/test.json\n",
            "100% 12217/12217 [00:15<00:00, 784.06it/s]\n",
            "07/03 02:56:39 PM: Processing file: /root/glue_data/edges/ontonotes/const/train.json\n",
            "07/03 02:57:17 PM:   saving to /root/glue_data/edges/ontonotes/const/pos/train.json and /root/glue_data/edges/ontonotes/const/nonterminal/train.json\n",
            "100% 115812/115812 [02:33<00:00, 755.90it/s]\n",
            "+ preproc_task /root/glue_data/edges/ontonotes/const\n",
            "+ TASK_DIR=/root/glue_data/edges/ontonotes/const\n",
            "+ python ./jiant/probing/get_edge_data_labels.py -o /root/glue_data/edges/ontonotes/const/labels.txt -i /root/glue_data/edges/ontonotes/const/conll-2012-test.json /root/glue_data/edges/ontonotes/const/development.json /root/glue_data/edges/ontonotes/const/test.json /root/glue_data/edges/ontonotes/const/train.json -s\n",
            "07/03 02:59:54 PM: Counting labels in /root/glue_data/edges/ontonotes/const/conll-2012-test.json\n",
            "9479it [00:01, 6916.22it/s]\n",
            "07/03 02:59:56 PM: Counting labels in /root/glue_data/edges/ontonotes/const/development.json\n",
            "15680it [00:02, 6723.81it/s]\n",
            "07/03 02:59:58 PM: Counting labels in /root/glue_data/edges/ontonotes/const/test.json\n",
            "12217it [00:02, 5740.59it/s]\n",
            "07/03 03:00:00 PM: Counting labels in /root/glue_data/edges/ontonotes/const/train.json\n",
            "115812it [00:17, 6748.65it/s]\n",
            "07/03 03:00:17 PM: 78 labels in total (0 special + 78 found)\n",
            "+ python ./jiant/probing/retokenize_edge_data.py -t bert-base-uncased /root/glue_data/edges/ontonotes/const/conll-2012-test.json /root/glue_data/edges/ontonotes/const/development.json /root/glue_data/edges/ontonotes/const/test.json /root/glue_data/edges/ontonotes/const/train.json\n",
            "07/03 03:00:24 PM: \tLoading Tokenizer MosesTokenizer\n",
            "07/03 03:00:24 PM: Processing file: /root/glue_data/edges/ontonotes/const/conll-2012-test.json\n",
            "07/03 03:00:25 PM:   saving to /root/glue_data/edges/ontonotes/const/conll-2012-test.json.retokenized.bert-base-uncased\n",
            "  0% 0/9479 [00:00<?, ?it/s]07/03 03:00:25 PM: \tLoading Tokenizer bert-base-uncased\n",
            "07/03 03:00:25 PM: \tLoading Tokenizer bert-base-uncased\n",
            "07/03 03:00:25 PM: \tLoading Tokenizer bert-base-uncased\n",
            "07/03 03:00:25 PM: \tLoading Tokenizer bert-base-uncased\n",
            "07/03 03:00:25 PM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "07/03 03:00:25 PM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "07/03 03:00:25 PM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "07/03 03:00:25 PM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "100% 9479/9479 [00:18<00:00, 518.90it/s]\n",
            "07/03 03:00:43 PM: Processing file: /root/glue_data/edges/ontonotes/const/development.json\n",
            "07/03 03:00:43 PM:   saving to /root/glue_data/edges/ontonotes/const/development.json.retokenized.bert-base-uncased\n",
            "100% 15680/15680 [00:30<00:00, 509.90it/s]\n",
            "07/03 03:01:14 PM: Processing file: /root/glue_data/edges/ontonotes/const/test.json\n",
            "07/03 03:01:14 PM:   saving to /root/glue_data/edges/ontonotes/const/test.json.retokenized.bert-base-uncased\n",
            "100% 12217/12217 [00:23<00:00, 523.75it/s]\n",
            "07/03 03:01:37 PM: Processing file: /root/glue_data/edges/ontonotes/const/train.json\n",
            "07/03 03:01:38 PM:   saving to /root/glue_data/edges/ontonotes/const/train.json.retokenized.bert-base-uncased\n",
            "100% 115812/115812 [03:46<00:00, 511.83it/s]\n",
            "+ python ./jiant/probing/convert_edge_data_to_tfrecord.py /root/glue_data/edges/ontonotes/const/conll-2012-test.json /root/glue_data/edges/ontonotes/const/development.json /root/glue_data/edges/ontonotes/const/test.json /root/glue_data/edges/ontonotes/const/train.json\n",
            "07/03 03:05:29 PM: Processing file: /root/glue_data/edges/ontonotes/const/conll-2012-test.json\n",
            "07/03 03:05:29 PM:   saving to /root/glue_data/edges/ontonotes/const/conll-2012-test.tfrecord\n",
            "WARNING:tensorflow:From ./jiant/probing/convert_edge_data_to_tfrecord.py:89: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n",
            "\n",
            "07/03 03:05:29 PM: From ./jiant/probing/convert_edge_data_to_tfrecord.py:89: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n",
            "\n",
            "9479it [00:06, 1554.41it/s]\n",
            "07/03 03:05:35 PM: Processing file: /root/glue_data/edges/ontonotes/const/development.json\n",
            "07/03 03:05:35 PM:   saving to /root/glue_data/edges/ontonotes/const/development.tfrecord\n",
            "15680it [00:10, 1541.86it/s]\n",
            "07/03 03:05:45 PM: Processing file: /root/glue_data/edges/ontonotes/const/test.json\n",
            "07/03 03:05:45 PM:   saving to /root/glue_data/edges/ontonotes/const/test.tfrecord\n",
            "12217it [00:07, 1582.61it/s]\n",
            "07/03 03:05:53 PM: Processing file: /root/glue_data/edges/ontonotes/const/train.json\n",
            "07/03 03:05:53 PM:   saving to /root/glue_data/edges/ontonotes/const/train.tfrecord\n",
            "115812it [01:15, 1535.88it/s]\n",
            "+ preproc_task /root/glue_data/edges/ontonotes/const/pos\n",
            "+ TASK_DIR=/root/glue_data/edges/ontonotes/const/pos\n",
            "+ python ./jiant/probing/get_edge_data_labels.py -o /root/glue_data/edges/ontonotes/const/pos/labels.txt -i /root/glue_data/edges/ontonotes/const/pos/conll-2012-test.json /root/glue_data/edges/ontonotes/const/pos/development.json /root/glue_data/edges/ontonotes/const/pos/test.json /root/glue_data/edges/ontonotes/const/pos/train.json -s\n",
            "07/03 03:07:10 PM: Counting labels in /root/glue_data/edges/ontonotes/const/pos/conll-2012-test.json\n",
            "9479it [00:00, 13549.18it/s]\n",
            "07/03 03:07:11 PM: Counting labels in /root/glue_data/edges/ontonotes/const/pos/development.json\n",
            "15680it [00:01, 13027.84it/s]\n",
            "07/03 03:07:12 PM: Counting labels in /root/glue_data/edges/ontonotes/const/pos/test.json\n",
            "12217it [00:00, 13273.09it/s]\n",
            "07/03 03:07:13 PM: Counting labels in /root/glue_data/edges/ontonotes/const/pos/train.json\n",
            "115812it [00:08, 13669.77it/s]\n",
            "07/03 03:07:22 PM: 48 labels in total (0 special + 48 found)\n",
            "+ python ./jiant/probing/retokenize_edge_data.py -t bert-base-uncased /root/glue_data/edges/ontonotes/const/pos/conll-2012-test.json /root/glue_data/edges/ontonotes/const/pos/development.json /root/glue_data/edges/ontonotes/const/pos/test.json /root/glue_data/edges/ontonotes/const/pos/train.json\n",
            "07/03 03:07:25 PM: \tLoading Tokenizer MosesTokenizer\n",
            "07/03 03:07:26 PM: Processing file: /root/glue_data/edges/ontonotes/const/pos/conll-2012-test.json\n",
            "07/03 03:07:26 PM:   saving to /root/glue_data/edges/ontonotes/const/pos/conll-2012-test.json.retokenized.bert-base-uncased\n",
            "  0% 0/9479 [00:00<?, ?it/s]07/03 03:07:26 PM: \tLoading Tokenizer bert-base-uncased\n",
            "07/03 03:07:26 PM: \tLoading Tokenizer bert-base-uncased\n",
            "07/03 03:07:26 PM: \tLoading Tokenizer bert-base-uncased\n",
            "07/03 03:07:26 PM: \tLoading Tokenizer bert-base-uncased\n",
            "07/03 03:07:26 PM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "07/03 03:07:26 PM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "07/03 03:07:26 PM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "07/03 03:07:26 PM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "100% 9479/9479 [00:15<00:00, 607.63it/s]\n",
            "07/03 03:07:41 PM: Processing file: /root/glue_data/edges/ontonotes/const/pos/development.json\n",
            "07/03 03:07:41 PM:   saving to /root/glue_data/edges/ontonotes/const/pos/development.json.retokenized.bert-base-uncased\n",
            "100% 15680/15680 [00:26<00:00, 598.73it/s]\n",
            "07/03 03:08:07 PM: Processing file: /root/glue_data/edges/ontonotes/const/pos/test.json\n",
            "07/03 03:08:07 PM:   saving to /root/glue_data/edges/ontonotes/const/pos/test.json.retokenized.bert-base-uncased\n",
            "100% 12217/12217 [00:20<00:00, 587.81it/s]\n",
            "07/03 03:08:28 PM: Processing file: /root/glue_data/edges/ontonotes/const/pos/train.json\n",
            "07/03 03:08:29 PM:   saving to /root/glue_data/edges/ontonotes/const/pos/train.json.retokenized.bert-base-uncased\n",
            "100% 115812/115812 [03:12<00:00, 601.60it/s]\n",
            "+ python ./jiant/probing/convert_edge_data_to_tfrecord.py /root/glue_data/edges/ontonotes/const/pos/conll-2012-test.json /root/glue_data/edges/ontonotes/const/pos/development.json /root/glue_data/edges/ontonotes/const/pos/test.json /root/glue_data/edges/ontonotes/const/pos/train.json\n",
            "07/03 03:11:45 PM: Processing file: /root/glue_data/edges/ontonotes/const/pos/conll-2012-test.json\n",
            "07/03 03:11:45 PM:   saving to /root/glue_data/edges/ontonotes/const/pos/conll-2012-test.tfrecord\n",
            "WARNING:tensorflow:From ./jiant/probing/convert_edge_data_to_tfrecord.py:89: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n",
            "\n",
            "07/03 03:11:45 PM: From ./jiant/probing/convert_edge_data_to_tfrecord.py:89: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n",
            "\n",
            "9479it [00:03, 2767.10it/s]\n",
            "07/03 03:11:49 PM: Processing file: /root/glue_data/edges/ontonotes/const/pos/development.json\n",
            "07/03 03:11:49 PM:   saving to /root/glue_data/edges/ontonotes/const/pos/development.tfrecord\n",
            "15680it [00:05, 2643.43it/s]\n",
            "07/03 03:11:55 PM: Processing file: /root/glue_data/edges/ontonotes/const/pos/test.json\n",
            "07/03 03:11:55 PM:   saving to /root/glue_data/edges/ontonotes/const/pos/test.tfrecord\n",
            "12217it [00:04, 2838.89it/s]\n",
            "07/03 03:11:59 PM: Processing file: /root/glue_data/edges/ontonotes/const/pos/train.json\n",
            "07/03 03:11:59 PM:   saving to /root/glue_data/edges/ontonotes/const/pos/train.tfrecord\n",
            "115812it [00:43, 2654.98it/s]\n",
            "+ preproc_task /root/glue_data/edges/ontonotes/const/nonterminal\n",
            "+ TASK_DIR=/root/glue_data/edges/ontonotes/const/nonterminal\n",
            "+ python ./jiant/probing/get_edge_data_labels.py -o /root/glue_data/edges/ontonotes/const/nonterminal/labels.txt -i /root/glue_data/edges/ontonotes/const/nonterminal/conll-2012-test.json /root/glue_data/edges/ontonotes/const/nonterminal/development.json /root/glue_data/edges/ontonotes/const/nonterminal/test.json /root/glue_data/edges/ontonotes/const/nonterminal/train.json -s\n",
            "07/03 03:12:45 PM: Counting labels in /root/glue_data/edges/ontonotes/const/nonterminal/conll-2012-test.json\n",
            "9479it [00:00, 13849.75it/s]\n",
            "07/03 03:12:46 PM: Counting labels in /root/glue_data/edges/ontonotes/const/nonterminal/development.json\n",
            "15680it [00:01, 14396.74it/s]\n",
            "07/03 03:12:47 PM: Counting labels in /root/glue_data/edges/ontonotes/const/nonterminal/test.json\n",
            "12217it [00:00, 15233.48it/s]\n",
            "07/03 03:12:47 PM: Counting labels in /root/glue_data/edges/ontonotes/const/nonterminal/train.json\n",
            "115812it [00:07, 14621.38it/s]\n",
            "07/03 03:12:55 PM: 30 labels in total (0 special + 30 found)\n",
            "+ python ./jiant/probing/retokenize_edge_data.py -t bert-base-uncased /root/glue_data/edges/ontonotes/const/nonterminal/conll-2012-test.json /root/glue_data/edges/ontonotes/const/nonterminal/development.json /root/glue_data/edges/ontonotes/const/nonterminal/test.json /root/glue_data/edges/ontonotes/const/nonterminal/train.json\n",
            "07/03 03:12:59 PM: \tLoading Tokenizer MosesTokenizer\n",
            "07/03 03:12:59 PM: Processing file: /root/glue_data/edges/ontonotes/const/nonterminal/conll-2012-test.json\n",
            "07/03 03:12:59 PM:   saving to /root/glue_data/edges/ontonotes/const/nonterminal/conll-2012-test.json.retokenized.bert-base-uncased\n",
            "  0% 0/9479 [00:00<?, ?it/s]07/03 03:12:59 PM: \tLoading Tokenizer bert-base-uncased\n",
            "07/03 03:12:59 PM: \tLoading Tokenizer bert-base-uncased\n",
            "07/03 03:12:59 PM: \tLoading Tokenizer bert-base-uncased\n",
            "07/03 03:12:59 PM: \tLoading Tokenizer bert-base-uncased\n",
            "07/03 03:13:00 PM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "07/03 03:13:00 PM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "07/03 03:13:00 PM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "07/03 03:13:00 PM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "100% 9479/9479 [00:15<00:00, 615.28it/s]\n",
            "07/03 03:13:15 PM: Processing file: /root/glue_data/edges/ontonotes/const/nonterminal/development.json\n",
            "07/03 03:13:15 PM:   saving to /root/glue_data/edges/ontonotes/const/nonterminal/development.json.retokenized.bert-base-uncased\n",
            "100% 15680/15680 [00:26<00:00, 602.66it/s]\n",
            "07/03 03:13:41 PM: Processing file: /root/glue_data/edges/ontonotes/const/nonterminal/test.json\n",
            "07/03 03:13:41 PM:   saving to /root/glue_data/edges/ontonotes/const/nonterminal/test.json.retokenized.bert-base-uncased\n",
            "100% 12217/12217 [00:19<00:00, 616.11it/s]\n",
            "07/03 03:14:01 PM: Processing file: /root/glue_data/edges/ontonotes/const/nonterminal/train.json\n",
            "07/03 03:14:01 PM:   saving to /root/glue_data/edges/ontonotes/const/nonterminal/train.json.retokenized.bert-base-uncased\n",
            "100% 115812/115812 [03:11<00:00, 603.96it/s]\n",
            "+ python ./jiant/probing/convert_edge_data_to_tfrecord.py /root/glue_data/edges/ontonotes/const/nonterminal/conll-2012-test.json /root/glue_data/edges/ontonotes/const/nonterminal/development.json /root/glue_data/edges/ontonotes/const/nonterminal/test.json /root/glue_data/edges/ontonotes/const/nonterminal/train.json\n",
            "07/03 03:17:18 PM: Processing file: /root/glue_data/edges/ontonotes/const/nonterminal/conll-2012-test.json\n",
            "07/03 03:17:18 PM:   saving to /root/glue_data/edges/ontonotes/const/nonterminal/conll-2012-test.tfrecord\n",
            "WARNING:tensorflow:From ./jiant/probing/convert_edge_data_to_tfrecord.py:89: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n",
            "\n",
            "07/03 03:17:18 PM: From ./jiant/probing/convert_edge_data_to_tfrecord.py:89: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n",
            "\n",
            "9479it [00:03, 3009.91it/s]\n",
            "07/03 03:17:21 PM: Processing file: /root/glue_data/edges/ontonotes/const/nonterminal/development.json\n",
            "07/03 03:17:21 PM:   saving to /root/glue_data/edges/ontonotes/const/nonterminal/development.tfrecord\n",
            "15680it [00:05, 2946.79it/s]\n",
            "07/03 03:17:26 PM: Processing file: /root/glue_data/edges/ontonotes/const/nonterminal/test.json\n",
            "07/03 03:17:26 PM:   saving to /root/glue_data/edges/ontonotes/const/nonterminal/test.tfrecord\n",
            "12217it [00:04, 3030.38it/s]\n",
            "07/03 03:17:30 PM: Processing file: /root/glue_data/edges/ontonotes/const/nonterminal/train.json\n",
            "07/03 03:17:30 PM:   saving to /root/glue_data/edges/ontonotes/const/nonterminal/train.tfrecord\n",
            "115812it [00:39, 2957.78it/s]\n",
            "+ preproc_task /root/glue_data/edges/ontonotes/coref\n",
            "+ TASK_DIR=/root/glue_data/edges/ontonotes/coref\n",
            "+ python ./jiant/probing/get_edge_data_labels.py -o /root/glue_data/edges/ontonotes/coref/labels.txt -i /root/glue_data/edges/ontonotes/coref/conll-2012-test.json /root/glue_data/edges/ontonotes/coref/development.json /root/glue_data/edges/ontonotes/coref/test.json /root/glue_data/edges/ontonotes/coref/train.json -s\n",
            "07/03 03:18:11 PM: Counting labels in /root/glue_data/edges/ontonotes/coref/conll-2012-test.json\n",
            "9479it [00:00, 76099.95it/s]\n",
            "07/03 03:18:11 PM: Counting labels in /root/glue_data/edges/ontonotes/coref/development.json\n",
            "15680it [00:00, 97483.54it/s]\n",
            "07/03 03:18:12 PM: Counting labels in /root/glue_data/edges/ontonotes/coref/test.json\n",
            "12217it [00:00, 93549.46it/s]\n",
            "07/03 03:18:12 PM: Counting labels in /root/glue_data/edges/ontonotes/coref/train.json\n",
            "115812it [00:01, 101929.37it/s]\n",
            "07/03 03:18:13 PM: 2 labels in total (0 special + 2 found)\n",
            "+ python ./jiant/probing/retokenize_edge_data.py -t bert-base-uncased /root/glue_data/edges/ontonotes/coref/conll-2012-test.json /root/glue_data/edges/ontonotes/coref/development.json /root/glue_data/edges/ontonotes/coref/test.json /root/glue_data/edges/ontonotes/coref/train.json\n",
            "07/03 03:18:17 PM: \tLoading Tokenizer MosesTokenizer\n",
            "07/03 03:18:17 PM: Processing file: /root/glue_data/edges/ontonotes/coref/conll-2012-test.json\n",
            "07/03 03:18:17 PM:   saving to /root/glue_data/edges/ontonotes/coref/conll-2012-test.json.retokenized.bert-base-uncased\n",
            "  0% 0/9479 [00:00<?, ?it/s]07/03 03:18:17 PM: \tLoading Tokenizer bert-base-uncased\n",
            "07/03 03:18:17 PM: \tLoading Tokenizer bert-base-uncased\n",
            "07/03 03:18:17 PM: \tLoading Tokenizer bert-base-uncased\n",
            "07/03 03:18:17 PM: \tLoading Tokenizer bert-base-uncased\n",
            "07/03 03:18:17 PM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "07/03 03:18:17 PM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "07/03 03:18:17 PM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "07/03 03:18:17 PM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "100% 9479/9479 [00:13<00:00, 695.71it/s]\n",
            "07/03 03:18:31 PM: Processing file: /root/glue_data/edges/ontonotes/coref/development.json\n",
            "07/03 03:18:31 PM:   saving to /root/glue_data/edges/ontonotes/coref/development.json.retokenized.bert-base-uncased\n",
            "100% 15680/15680 [00:23<00:00, 665.29it/s]\n",
            "07/03 03:18:54 PM: Processing file: /root/glue_data/edges/ontonotes/coref/test.json\n",
            "07/03 03:18:54 PM:   saving to /root/glue_data/edges/ontonotes/coref/test.json.retokenized.bert-base-uncased\n",
            "100% 12217/12217 [00:17<00:00, 703.14it/s]\n",
            "07/03 03:19:12 PM: Processing file: /root/glue_data/edges/ontonotes/coref/train.json\n",
            "07/03 03:19:12 PM:   saving to /root/glue_data/edges/ontonotes/coref/train.json.retokenized.bert-base-uncased\n",
            "100% 115812/115812 [02:47<00:00, 691.84it/s]\n",
            "+ python ./jiant/probing/convert_edge_data_to_tfrecord.py /root/glue_data/edges/ontonotes/coref/conll-2012-test.json /root/glue_data/edges/ontonotes/coref/development.json /root/glue_data/edges/ontonotes/coref/test.json /root/glue_data/edges/ontonotes/coref/train.json\n",
            "07/03 03:22:03 PM: Processing file: /root/glue_data/edges/ontonotes/coref/conll-2012-test.json\n",
            "07/03 03:22:03 PM:   saving to /root/glue_data/edges/ontonotes/coref/conll-2012-test.tfrecord\n",
            "WARNING:tensorflow:From ./jiant/probing/convert_edge_data_to_tfrecord.py:89: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n",
            "\n",
            "07/03 03:22:03 PM: From ./jiant/probing/convert_edge_data_to_tfrecord.py:89: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n",
            "\n",
            "9479it [00:00, 10231.11it/s]\n",
            "07/03 03:22:04 PM: Processing file: /root/glue_data/edges/ontonotes/coref/development.json\n",
            "07/03 03:22:04 PM:   saving to /root/glue_data/edges/ontonotes/coref/development.tfrecord\n",
            "15680it [00:01, 13377.06it/s]\n",
            "07/03 03:22:05 PM: Processing file: /root/glue_data/edges/ontonotes/coref/test.json\n",
            "07/03 03:22:05 PM:   saving to /root/glue_data/edges/ontonotes/coref/test.tfrecord\n",
            "12217it [00:01, 11737.09it/s]\n",
            "07/03 03:22:06 PM: Processing file: /root/glue_data/edges/ontonotes/coref/train.json\n",
            "07/03 03:22:06 PM:   saving to /root/glue_data/edges/ontonotes/coref/train.tfrecord\n",
            "115812it [00:08, 13188.08it/s]\n",
            "+ preproc_task /root/glue_data/edges/ontonotes/ner\n",
            "+ TASK_DIR=/root/glue_data/edges/ontonotes/ner\n",
            "+ python ./jiant/probing/get_edge_data_labels.py -o /root/glue_data/edges/ontonotes/ner/labels.txt -i /root/glue_data/edges/ontonotes/ner/conll-2012-test.json /root/glue_data/edges/ontonotes/ner/development.json /root/glue_data/edges/ontonotes/ner/test.json /root/glue_data/edges/ontonotes/ner/train.json -s\n",
            "07/03 03:22:17 PM: Counting labels in /root/glue_data/edges/ontonotes/ner/conll-2012-test.json\n",
            "9479it [00:00, 138404.53it/s]\n",
            "07/03 03:22:17 PM: Counting labels in /root/glue_data/edges/ontonotes/ner/development.json\n",
            "15680it [00:00, 138296.92it/s]\n",
            "07/03 03:22:17 PM: Counting labels in /root/glue_data/edges/ontonotes/ner/test.json\n",
            "12217it [00:00, 145834.75it/s]\n",
            "07/03 03:22:17 PM: Counting labels in /root/glue_data/edges/ontonotes/ner/train.json\n",
            "115812it [00:00, 131220.64it/s]\n",
            "07/03 03:22:18 PM: 18 labels in total (0 special + 18 found)\n",
            "+ python ./jiant/probing/retokenize_edge_data.py -t bert-base-uncased /root/glue_data/edges/ontonotes/ner/conll-2012-test.json /root/glue_data/edges/ontonotes/ner/development.json /root/glue_data/edges/ontonotes/ner/test.json /root/glue_data/edges/ontonotes/ner/train.json\n",
            "07/03 03:22:22 PM: \tLoading Tokenizer MosesTokenizer\n",
            "07/03 03:22:22 PM: Processing file: /root/glue_data/edges/ontonotes/ner/conll-2012-test.json\n",
            "07/03 03:22:22 PM:   saving to /root/glue_data/edges/ontonotes/ner/conll-2012-test.json.retokenized.bert-base-uncased\n",
            "  0% 0/9479 [00:00<?, ?it/s]07/03 03:22:22 PM: \tLoading Tokenizer bert-base-uncased\n",
            "07/03 03:22:22 PM: \tLoading Tokenizer bert-base-uncased\n",
            "07/03 03:22:22 PM: \tLoading Tokenizer bert-base-uncased\n",
            "07/03 03:22:22 PM: \tLoading Tokenizer bert-base-uncased\n",
            "07/03 03:22:22 PM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "07/03 03:22:22 PM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "07/03 03:22:22 PM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "07/03 03:22:22 PM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "100% 9479/9479 [00:12<00:00, 737.06it/s]\n",
            "07/03 03:22:35 PM: Processing file: /root/glue_data/edges/ontonotes/ner/development.json\n",
            "07/03 03:22:35 PM:   saving to /root/glue_data/edges/ontonotes/ner/development.json.retokenized.bert-base-uncased\n",
            "100% 15680/15680 [00:21<00:00, 727.72it/s]\n",
            "07/03 03:22:56 PM: Processing file: /root/glue_data/edges/ontonotes/ner/test.json\n",
            "07/03 03:22:56 PM:   saving to /root/glue_data/edges/ontonotes/ner/test.json.retokenized.bert-base-uncased\n",
            "100% 12217/12217 [00:17<00:00, 704.79it/s]\n",
            "07/03 03:23:14 PM: Processing file: /root/glue_data/edges/ontonotes/ner/train.json\n",
            "07/03 03:23:14 PM:   saving to /root/glue_data/edges/ontonotes/ner/train.json.retokenized.bert-base-uncased\n",
            "100% 115812/115812 [02:39<00:00, 724.31it/s]\n",
            "+ python ./jiant/probing/convert_edge_data_to_tfrecord.py /root/glue_data/edges/ontonotes/ner/conll-2012-test.json /root/glue_data/edges/ontonotes/ner/development.json /root/glue_data/edges/ontonotes/ner/test.json /root/glue_data/edges/ontonotes/ner/train.json\n",
            "07/03 03:25:58 PM: Processing file: /root/glue_data/edges/ontonotes/ner/conll-2012-test.json\n",
            "07/03 03:25:58 PM:   saving to /root/glue_data/edges/ontonotes/ner/conll-2012-test.tfrecord\n",
            "WARNING:tensorflow:From ./jiant/probing/convert_edge_data_to_tfrecord.py:89: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n",
            "\n",
            "07/03 03:25:58 PM: From ./jiant/probing/convert_edge_data_to_tfrecord.py:89: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n",
            "\n",
            "9479it [00:00, 15665.71it/s]\n",
            "07/03 03:25:58 PM: Processing file: /root/glue_data/edges/ontonotes/ner/development.json\n",
            "07/03 03:25:58 PM:   saving to /root/glue_data/edges/ontonotes/ner/development.tfrecord\n",
            "15680it [00:01, 15533.87it/s]\n",
            "07/03 03:25:59 PM: Processing file: /root/glue_data/edges/ontonotes/ner/test.json\n",
            "07/03 03:25:59 PM:   saving to /root/glue_data/edges/ontonotes/ner/test.tfrecord\n",
            "12217it [00:00, 16086.67it/s]\n",
            "07/03 03:26:00 PM: Processing file: /root/glue_data/edges/ontonotes/ner/train.json\n",
            "07/03 03:26:00 PM:   saving to /root/glue_data/edges/ontonotes/ner/train.tfrecord\n",
            "115812it [00:07, 16187.61it/s]\n",
            "+ preproc_task /root/glue_data/edges/ontonotes/srl\n",
            "+ TASK_DIR=/root/glue_data/edges/ontonotes/srl\n",
            "+ python ./jiant/probing/get_edge_data_labels.py -o /root/glue_data/edges/ontonotes/srl/labels.txt -i /root/glue_data/edges/ontonotes/srl/conll-2012-test.json /root/glue_data/edges/ontonotes/srl/development.json /root/glue_data/edges/ontonotes/srl/test.json /root/glue_data/edges/ontonotes/srl/train.json -s\n",
            "07/03 03:26:09 PM: Counting labels in /root/glue_data/edges/ontonotes/srl/conll-2012-test.json\n",
            "24462it [00:00, 70989.36it/s]\n",
            "07/03 03:26:10 PM: Counting labels in /root/glue_data/edges/ontonotes/srl/development.json\n",
            "35297it [00:00, 84156.31it/s]\n",
            "07/03 03:26:10 PM: Counting labels in /root/glue_data/edges/ontonotes/srl/test.json\n",
            "26715it [00:00, 84008.59it/s]\n",
            "07/03 03:26:10 PM: Counting labels in /root/glue_data/edges/ontonotes/srl/train.json\n",
            "253070it [00:03, 81734.31it/s]\n",
            "07/03 03:26:13 PM: 66 labels in total (0 special + 66 found)\n",
            "+ python ./jiant/probing/retokenize_edge_data.py -t bert-base-uncased /root/glue_data/edges/ontonotes/srl/conll-2012-test.json /root/glue_data/edges/ontonotes/srl/development.json /root/glue_data/edges/ontonotes/srl/test.json /root/glue_data/edges/ontonotes/srl/train.json\n",
            "07/03 03:26:17 PM: \tLoading Tokenizer MosesTokenizer\n",
            "07/03 03:26:17 PM: Processing file: /root/glue_data/edges/ontonotes/srl/conll-2012-test.json\n",
            "07/03 03:26:17 PM:   saving to /root/glue_data/edges/ontonotes/srl/conll-2012-test.json.retokenized.bert-base-uncased\n",
            "  0% 0/24462 [00:00<?, ?it/s]07/03 03:26:17 PM: \tLoading Tokenizer bert-base-uncased\n",
            "07/03 03:26:17 PM: \tLoading Tokenizer bert-base-uncased\n",
            "07/03 03:26:17 PM: \tLoading Tokenizer bert-base-uncased\n",
            "07/03 03:26:17 PM: \tLoading Tokenizer bert-base-uncased\n",
            "07/03 03:26:18 PM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "07/03 03:26:18 PM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "07/03 03:26:18 PM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "07/03 03:26:18 PM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "100% 24462/24462 [00:41<00:00, 593.72it/s]\n",
            "07/03 03:26:58 PM: Processing file: /root/glue_data/edges/ontonotes/srl/development.json\n",
            "07/03 03:26:58 PM:   saving to /root/glue_data/edges/ontonotes/srl/development.json.retokenized.bert-base-uncased\n",
            "100% 35297/35297 [01:00<00:00, 579.63it/s]\n",
            "07/03 03:27:59 PM: Processing file: /root/glue_data/edges/ontonotes/srl/test.json\n",
            "07/03 03:27:59 PM:   saving to /root/glue_data/edges/ontonotes/srl/test.json.retokenized.bert-base-uncased\n",
            "100% 26715/26715 [00:45<00:00, 584.68it/s]\n",
            "07/03 03:28:45 PM: Processing file: /root/glue_data/edges/ontonotes/srl/train.json\n",
            "07/03 03:28:45 PM:   saving to /root/glue_data/edges/ontonotes/srl/train.json.retokenized.bert-base-uncased\n",
            "100% 253070/253070 [07:26<00:00, 566.16it/s]\n",
            "+ python ./jiant/probing/convert_edge_data_to_tfrecord.py /root/glue_data/edges/ontonotes/srl/conll-2012-test.json /root/glue_data/edges/ontonotes/srl/development.json /root/glue_data/edges/ontonotes/srl/test.json /root/glue_data/edges/ontonotes/srl/train.json\n",
            "07/03 03:36:16 PM: Processing file: /root/glue_data/edges/ontonotes/srl/conll-2012-test.json\n",
            "07/03 03:36:16 PM:   saving to /root/glue_data/edges/ontonotes/srl/conll-2012-test.tfrecord\n",
            "WARNING:tensorflow:From ./jiant/probing/convert_edge_data_to_tfrecord.py:89: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n",
            "\n",
            "07/03 03:36:16 PM: From ./jiant/probing/convert_edge_data_to_tfrecord.py:89: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n",
            "\n",
            "24462it [00:02, 10725.39it/s]\n",
            "07/03 03:36:19 PM: Processing file: /root/glue_data/edges/ontonotes/srl/development.json\n",
            "07/03 03:36:19 PM:   saving to /root/glue_data/edges/ontonotes/srl/development.tfrecord\n",
            "35297it [00:03, 10781.53it/s]\n",
            "07/03 03:36:22 PM: Processing file: /root/glue_data/edges/ontonotes/srl/test.json\n",
            "07/03 03:36:22 PM:   saving to /root/glue_data/edges/ontonotes/srl/test.tfrecord\n",
            "26715it [00:02, 11011.91it/s]\n",
            "07/03 03:36:24 PM: Processing file: /root/glue_data/edges/ontonotes/srl/train.json\n",
            "07/03 03:36:24 PM:   saving to /root/glue_data/edges/ontonotes/srl/train.tfrecord\n",
            "253070it [00:23, 10812.49it/s]\n",
            "+ get_spr_dpr\n",
            "./jiant/probing/get_and_process_all_data.sh: line 69: PATH_TO_SPR1_RUDINGER: unbound variable\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZH_WEgbCxYkU",
        "outputId": "d28cd1a8-1be2-47aa-952a-acbffc6cc1c2"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ZkVSxO2xnlQ"
      },
      "source": [
        "!cp -r /root /content/drive/MyDrive"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}