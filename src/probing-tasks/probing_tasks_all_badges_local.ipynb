{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "reproduce-probing-tasks.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.10 64-bit ('.bert_venv': venv)"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.10"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "99059d111a3f41b1ae5ca7b6371d1f18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_e9e79afa25394d8988591fd339e841f8",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_0a6348b453914258a8c73cb9b9ae3c69",
              "IPY_MODEL_d2f59c985c8e4159a65224bc4bee7234"
            ]
          }
        },
        "e9e79afa25394d8988591fd339e841f8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0a6348b453914258a8c73cb9b9ae3c69": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_499fba813ecf4fef8c11ffaad5c9a848",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 477,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 477,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_bdcb06ddfcda491393808269edb3563d"
          }
        },
        "d2f59c985c8e4159a65224bc4bee7234": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b9482bebc9ad4d82b909388329729674",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 477/477 [00:13&lt;00:00, 36.6B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_253c5f7e6d9141e285a7b9e0be725970"
          }
        },
        "499fba813ecf4fef8c11ffaad5c9a848": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "bdcb06ddfcda491393808269edb3563d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b9482bebc9ad4d82b909388329729674": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "253c5f7e6d9141e285a7b9e0be725970": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f20a722e76394a8894126a7dd914cfb4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_e7678370356244e4862d891ebd654fd7",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_b77ef787f6bc446096ee7a1000baf08f",
              "IPY_MODEL_c0ed4299f2c64dfeb66999aee98c2a89"
            ]
          }
        },
        "e7678370356244e4862d891ebd654fd7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b77ef787f6bc446096ee7a1000baf08f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_132475dfdef348d1b9fd9167e5b42d88",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 437983344,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 437983344,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_680e522ccde446eb8bc7e8e3263b7f7c"
          }
        },
        "c0ed4299f2c64dfeb66999aee98c2a89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e0c28338bb6c49b2adccbc815138204b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 438M/438M [00:09&lt;00:00, 44.7MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6cc12a33148545dd9a0391556f373837"
          }
        },
        "132475dfdef348d1b9fd9167e5b42d88": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "680e522ccde446eb8bc7e8e3263b7f7c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e0c28338bb6c49b2adccbc815138204b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6cc12a33148545dd9a0391556f373837": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "77a0e1a6a5b645fd9bb19e392a36fe26": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_7778547971784683b9ca984b43c8360b",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_217e3375c6364785916345f01ed67056",
              "IPY_MODEL_9805b9a420b349038db4ba01b53c3a8b"
            ]
          }
        },
        "7778547971784683b9ca984b43c8360b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "217e3375c6364785916345f01ed67056": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_384ed451e4944a41a38ed259e2bd584b",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_caba545fab574101aaffd792d7a68494"
          }
        },
        "9805b9a420b349038db4ba01b53c3a8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_f70ce67f216541a2b4c8c22e16ae7d15",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 232k/232k [00:01&lt;00:00, 204kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_eecc2ed8018f4d7d886cf9626ae69048"
          }
        },
        "384ed451e4944a41a38ed259e2bd584b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "caba545fab574101aaffd792d7a68494": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f70ce67f216541a2b4c8c22e16ae7d15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "eecc2ed8018f4d7d886cf9626ae69048": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f59c5a0eabbf4b22ba0848e93048c100": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_f77e914a0a9645a0b685a81ad0a84ec9",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_79cb08466f694d01b1b766a7388fc511",
              "IPY_MODEL_3fba4a339539451d839f143447ab583e"
            ]
          }
        },
        "f77e914a0a9645a0b685a81ad0a84ec9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "79cb08466f694d01b1b766a7388fc511": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_642a0baeb8a743caa35f363731006262",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 112,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 112,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_11acf36d77ca4402a77f89656e82fcc7"
          }
        },
        "3fba4a339539451d839f143447ab583e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_53a121e3fb144667be7ddf855bebd26d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 112/112 [00:00&lt;00:00, 117B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d5da998587cd47f3ae61067f411d6649"
          }
        },
        "642a0baeb8a743caa35f363731006262": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "11acf36d77ca4402a77f89656e82fcc7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "53a121e3fb144667be7ddf855bebd26d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d5da998587cd47f3ae61067f411d6649": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3f8346cd7613432bbb91f4543db60057": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_008a57b8559b49ee9b06ccb1aac4eb28",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_61b77be9538c4a649cc9ece7ae362367",
              "IPY_MODEL_051f9eb69d0f469e9b84328152102a3f"
            ]
          }
        },
        "008a57b8559b49ee9b06ccb1aac4eb28": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "61b77be9538c4a649cc9ece7ae362367": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_7e3712ffa45a40deaf0c7f8e44f0b954",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 48,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 48,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1170bf0eadc2415d8920e5978ff06372"
          }
        },
        "051f9eb69d0f469e9b84328152102a3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_8d309c73287e4bc0adfc0676ca41f975",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 48.0/48.0 [00:00&lt;00:00, 137B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_756c49f41fb347f8ad8bba78ab4a1518"
          }
        },
        "7e3712ffa45a40deaf0c7f8e44f0b954": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1170bf0eadc2415d8920e5978ff06372": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8d309c73287e4bc0adfc0676ca41f975": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "756c49f41fb347f8ad8bba78ab4a1518": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "28c1ea452d9b47ddb259855c7dd5166d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_877e265441eb4113b820e01a7ed96a51",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_ab90cb7773f94f19bfd1231ca07eee35",
              "IPY_MODEL_0527b8652d634d36a9b718efa7364cf6"
            ]
          }
        },
        "877e265441eb4113b820e01a7ed96a51": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ab90cb7773f94f19bfd1231ca07eee35": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_819216839be54566bfd46220f4d8e907",
            "_dom_classes": [],
            "description": "Tokenizing: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 160,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 160,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_15c6494d0cbc4e6a9d7084155e68ee0a"
          }
        },
        "0527b8652d634d36a9b718efa7364cf6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_24cb04b0935f40a3b3e4a9bb4ec9c9cc",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 160/160 [00:02&lt;00:00, 71.96it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4af10d12632c419dacf93b52e5b66576"
          }
        },
        "819216839be54566bfd46220f4d8e907": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "15c6494d0cbc4e6a9d7084155e68ee0a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "24cb04b0935f40a3b3e4a9bb4ec9c9cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4af10d12632c419dacf93b52e5b66576": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "df80b86f99c94fdcaad7a8ec2af40c84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_33af4d43fc7547278130c4c4a8d602b0",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_3807b4ab76184d9d9b7f1ae6ed4dc2e7",
              "IPY_MODEL_279ca3584e174f9ab6ac5bac897d3de9"
            ]
          }
        },
        "33af4d43fc7547278130c4c4a8d602b0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3807b4ab76184d9d9b7f1ae6ed4dc2e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_23d867bd8a8b458e9c47af8e93b368ec",
            "_dom_classes": [],
            "description": "Tokenizing: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 21,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 21,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5c9896d371dc451cb9f60505e73934e0"
          }
        },
        "279ca3584e174f9ab6ac5bac897d3de9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_44c410d1f5b1446fa3011750e0124cf6",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 21/21 [00:00&lt;00:00, 134.62it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f505c396c17340ca87054ac668f4841c"
          }
        },
        "23d867bd8a8b458e9c47af8e93b368ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5c9896d371dc451cb9f60505e73934e0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "44c410d1f5b1446fa3011750e0124cf6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f505c396c17340ca87054ac668f4841c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    },
    "interpreter": {
      "hash": "b6f5aa6c83ab6bb418a26169cae6e1f2aa26236cb3670b5bbe663715684b9fc3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ydn1RkgqV33e"
      },
      "source": [
        "# **Probing Tasks - How Does BERT Answer Questions?**\n",
        "In this notebook, we will carry out the following badges:\n",
        "\n",
        "1.   reproduce the probing tasks:\n",
        "  *   NEL, REL, COREF on the Ontonotes dataset\n",
        "  *   QUES on TREC-10 dataset\n",
        "  *   SUP on the Squad dataset\n",
        "\n",
        "\n",
        "2.   experiment with BERT (**todo:** which type?) trained on (**todo:** which dataset?) dataset\n",
        "3.   experiment with T5 model fine-tuned on Squad\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1iV-pKNjJDS"
      },
      "source": [
        "# **1. Reproduce the probing tasks**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yg6j-eMFjv_T"
      },
      "source": [
        "## **1.1 Use modified `jiant` library**\n",
        "We modified some codes of the [original jiant library](https://github.com/nyu-mll/jiant) in order to achieve our desired functions that aren't supported by `jiant`. For more details please see our pdf report.\n",
        "\n",
        "First, we will clone the modified jiant and install libraries we need for this code. We assume that this Notebook is run in a local clone of the /How-Does-Bert-Answer-QA-DLP2021/ repository"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6oaJe7PeGpDE"
      },
      "source": [
        "%cd /src/probing-tasks/jiant\n",
        "!pip install -r requirements-no-torch.txt\n",
        "!pip install --no-deps -e ./"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkvpqthuQTZe"
      },
      "source": [
        "Restart runtime after installing libs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFXf-RICoEqh"
      },
      "source": [
        "## **1.2 Set up the Edge Probing tasks**\n",
        "After preprocessing and generating the Edge Probing data for all the tasks (see report for details), these data was uploaded to our github and will be used here. Next, we will create the corresponding task configs.\n",
        "\n",
        "Because the tasks QUES and SUP are not supported by jiant, we added new task QUES to the jiant library (see report). The task SUP has the same jiant format structure as COREF, therefore we will reuse the default COREF task in jiant to probe SUP task.\n",
        "\n",
        "Make sure to change the path to your local repository accordingly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WptdOn-EsMA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0fe2e94e-0785-4368-fe7a-9e262fec4911"
      },
      "source": [
        "%cd /home/peter/Documents/Deep Learning/How-Does-Bert-Answer-QA-DLP2021/src/probing-tasks/jiant/\n",
        "\n",
        "import jiant.utils.python.io as py_io\n",
        "import jiant.utils.display as display\n",
        "import os\n",
        "\n",
        "def init_task_config(task_name):\n",
        "  jiant_task = task_name\n",
        "  if(task_name == \"sup-squad\" or task_name == \"sup-babi\"):\n",
        "    jiant_task = \"coref\"  # use coref task to probe supporting facts task because of the analog structure of jiant EP json format\n",
        "\n",
        "  os.makedirs(\"../tasks/configs/\", exist_ok=True)\n",
        "  os.makedirs(f\"../tasks/data/{task_name}\", exist_ok=True)\n",
        "\n",
        "  py_io.write_json({\n",
        "    \"task\": jiant_task,\n",
        "    \"paths\": {\n",
        "      \"train\": f\"../data/{task_name}/train.jsonl\",\n",
        "      \"val\":   f\"../data/{task_name}/val.jsonl\",\n",
        "    },\n",
        "    \"name\": jiant_task\n",
        "  }, f\"../tasks/configs/{task_name}_config.json\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/home/peter/Documents/Deep Learning/How-Does-Bert-Answer-QA-DLP2021/src/probing-tasks/jiant\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0OVplH1gSFW"
      },
      "source": [
        "task_names = [\n",
        "              \"ner\", \n",
        "              #\"semeval\", \n",
        "              #\"coref\",     \n",
        "              #\"sup-squad\", \n",
        "              #\"ques\"\n",
        "              #\"sup-babi\",\n",
        "              #\"sup-hotpot\",\n",
        "             ]  \n",
        "\n",
        "for task_name in task_names:\n",
        "  init_task_config(task_name)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CZBFmqsr2A4"
      },
      "source": [
        "## **1.3 Download BERT models**\n",
        "Next, we download a `bert-base-uncased` and a `bert-base-uncased-squad-v1` model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kICMJQvNllO6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365,
          "referenced_widgets": [
            "99059d111a3f41b1ae5ca7b6371d1f18",
            "e9e79afa25394d8988591fd339e841f8",
            "0a6348b453914258a8c73cb9b9ae3c69",
            "d2f59c985c8e4159a65224bc4bee7234",
            "499fba813ecf4fef8c11ffaad5c9a848",
            "bdcb06ddfcda491393808269edb3563d",
            "b9482bebc9ad4d82b909388329729674",
            "253c5f7e6d9141e285a7b9e0be725970",
            "f20a722e76394a8894126a7dd914cfb4",
            "e7678370356244e4862d891ebd654fd7",
            "b77ef787f6bc446096ee7a1000baf08f",
            "c0ed4299f2c64dfeb66999aee98c2a89",
            "132475dfdef348d1b9fd9167e5b42d88",
            "680e522ccde446eb8bc7e8e3263b7f7c",
            "e0c28338bb6c49b2adccbc815138204b",
            "6cc12a33148545dd9a0391556f373837",
            "77a0e1a6a5b645fd9bb19e392a36fe26",
            "7778547971784683b9ca984b43c8360b",
            "217e3375c6364785916345f01ed67056",
            "9805b9a420b349038db4ba01b53c3a8b",
            "384ed451e4944a41a38ed259e2bd584b",
            "caba545fab574101aaffd792d7a68494",
            "f70ce67f216541a2b4c8c22e16ae7d15",
            "eecc2ed8018f4d7d886cf9626ae69048",
            "f59c5a0eabbf4b22ba0848e93048c100",
            "f77e914a0a9645a0b685a81ad0a84ec9",
            "79cb08466f694d01b1b766a7388fc511",
            "3fba4a339539451d839f143447ab583e",
            "642a0baeb8a743caa35f363731006262",
            "11acf36d77ca4402a77f89656e82fcc7",
            "53a121e3fb144667be7ddf855bebd26d",
            "d5da998587cd47f3ae61067f411d6649",
            "3f8346cd7613432bbb91f4543db60057",
            "008a57b8559b49ee9b06ccb1aac4eb28",
            "61b77be9538c4a649cc9ece7ae362367",
            "051f9eb69d0f469e9b84328152102a3f",
            "7e3712ffa45a40deaf0c7f8e44f0b954",
            "1170bf0eadc2415d8920e5978ff06372",
            "8d309c73287e4bc0adfc0676ca41f975",
            "756c49f41fb347f8ad8bba78ab4a1518"
          ]
        },
        "outputId": "3854617a-fcba-4244-c1e9-818cafff4093"
      },
      "source": [
        "import jiant.proj.main.export_model as export_model\n",
        "\n",
        "models = [\n",
        "          \"bert-base-uncased\", \n",
        "          \"csarron/bert-base-uncased-squad-v1\"\n",
        "          ]\n",
        "          \n",
        "for model in models:\n",
        "  export_model.export_model(\n",
        "      hf_pretrained_model_name_or_path=model,\n",
        "      output_base_path=f\"../tasks/models/{model}\",\n",
        "  )\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForPreTraining were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['cls.predictions.decoder.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of the model checkpoint at csarron/bert-base-uncased-squad-v1 were not used when initializing BertForPreTraining: ['qa_outputs.weight', 'qa_outputs.bias']\n",
            "- This IS expected if you are initializing BertForPreTraining from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForPreTraining from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForPreTraining were not initialized from the model checkpoint at csarron/bert-base-uncased-squad-v1 and are newly initialized: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3I4NLR9HteB3"
      },
      "source": [
        "## **1.4 Tokenize and cache**\n",
        "With the model and data ready, we can now tokenize and cache the inputs features for our task. This converts the input examples to tokenized features ready to be consumed by the model, and saved them to disk in chunks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wq2CmWcJRxK_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165,
          "referenced_widgets": [
            "28c1ea452d9b47ddb259855c7dd5166d",
            "877e265441eb4113b820e01a7ed96a51",
            "ab90cb7773f94f19bfd1231ca07eee35",
            "0527b8652d634d36a9b718efa7364cf6",
            "819216839be54566bfd46220f4d8e907",
            "15c6494d0cbc4e6a9d7084155e68ee0a",
            "24cb04b0935f40a3b3e4a9bb4ec9c9cc",
            "4af10d12632c419dacf93b52e5b66576",
            "df80b86f99c94fdcaad7a8ec2af40c84",
            "33af4d43fc7547278130c4c4a8d602b0",
            "3807b4ab76184d9d9b7f1ae6ed4dc2e7",
            "279ca3584e174f9ab6ac5bac897d3de9",
            "23d867bd8a8b458e9c47af8e93b368ec",
            "5c9896d371dc451cb9f60505e73934e0",
            "44c410d1f5b1446fa3011750e0124cf6",
            "f505c396c17340ca87054ac668f4841c"
          ]
        },
        "outputId": "e7f0d083-1076-4f14-84f9-0048e3f97ce1",
        "tags": []
      },
      "source": [
        "import jiant.shared.caching as caching\n",
        "import jiant.proj.main.tokenize_and_cache as tokenize_and_cache\n",
        "\n",
        "# Tokenize and cache each task\n",
        "def tokenize(task_names, models):\n",
        "  for task_name in task_names:\n",
        "    for model in models:\n",
        "      tokenize_and_cache.main(tokenize_and_cache.RunConfiguration(\n",
        "          task_config_path=f\"../tasks/configs/{task_name}_config.json\",\n",
        "          hf_pretrained_model_name_or_path=model,\n",
        "          output_dir=f\"../tasks/cache/{task_name}\",\n",
        "          phases=[\"train\", \"val\"],\n",
        "          max_seq_length=384,\n",
        "      ))\n",
        "\n",
        "tokenize(task_names, models)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NerTask\n",
            "  [train]: ../configs/../data/ner/train.jsonl\n",
            "  [val]: ../configs/../data/ner/val.jsonl\n",
            "Tokenizing: 100%|██████████| 6256/6256 [00:49<00:00, 126.51it/s]\n",
            "Tokenizing: 100%|██████████| 1201/1201 [00:05<00:00, 206.87it/s]\n",
            "NerTask\n",
            "  [train]: ../configs/../data/ner/train.jsonl\n",
            "  [val]: ../configs/../data/ner/val.jsonl\n",
            "Tokenizing: 100%|██████████| 6256/6256 [00:47<00:00, 130.61it/s]\n",
            "Tokenizing: 100%|██████████| 1201/1201 [00:05<00:00, 213.98it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlyEeBQOtwrv"
      },
      "source": [
        "We can inspect the first examples of the first chunk of each task."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AjUks1IQClLO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a52f4415-e6fd-4147-cd91-bbfa18847772"
      },
      "source": [
        "row = caching.ChunkedFilesDataCache(f\"../tasks/cache/{task_names[0]}/train\").load_chunk(0)[0][\"data_row\"]\n",
        "print(row.input_ids)\n",
        "print(row.tokens)\n",
        "print(row.spans)\n",
        "print(row.tokens[row.spans[0][0]: row.spans[0][1]+1])\n",
        "#print(row.tokens[row.spans[1][0]: row.spans[1][1]+1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[  101 24111  1005  1055  2197  2420  1998  2054  2003  2056  2002  2001\n",
            "  2066   102     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0]\n",
            "['[CLS]', 'saddam', \"'\", 's', 'last', 'days', 'and', 'what', 'is', 'said', 'he', 'was', 'like', '[SEP]']\n",
            "[[ 1  3]\n",
            " [10 10]]\n",
            "['saddam', \"'\", 's']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWb2rZ49ukZy"
      },
      "source": [
        "## **1.4 Write a run config**\n",
        "Here we are going to write what we call a `jiant_task_config`. This configuration file basically defines a lot of the subtleties of our training pipeline, such as what tasks we will train on, do evaluation on, batch size for each task. We use a helper `Configurator` to write out a `jiant_task_container_config`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQPixTeAClQD"
      },
      "source": [
        "import jiant.proj.main.scripts.configurator as configurator\n",
        "\n",
        "def create_jiant_task_config(task_name):\n",
        "  jiant_run_config = configurator.SimpleAPIMultiTaskConfigurator(\n",
        "      task_config_base_path=\"../tasks/configs\",\n",
        "      task_cache_base_path=\"../tasks/cache\",\n",
        "      train_task_name_list=[task_name],\n",
        "      val_task_name_list=[task_name],\n",
        "      train_batch_size=8,\n",
        "      eval_batch_size=2,\n",
        "      epochs=5,\n",
        "      num_gpus=1,\n",
        "  ).create_config()\n",
        "  os.makedirs(\"../tasks/run_configs/\", exist_ok=True)\n",
        "  py_io.write_json(jiant_run_config, f\"../tasks/run_configs/{task_name}_run_config.json\")\n",
        "  #display.show_json(jiant_run_config)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjTi5usdvdKo"
      },
      "source": [
        "## **1.5 Start training**\n",
        "We will run probing each layers of each model for each task and the visualize the macro averaged F1 over all layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yyZbu_KjClfE"
      },
      "source": [
        "# You may restart runtime when you modify the jiant source code\n",
        "\n",
        "import jiant.proj.main.runscript as main_runscript\n",
        "\n",
        "def run_probing_task(task_name, model_name=\"bert-base-uncased\", num_layers=1):\n",
        "  run_args = main_runscript.RunConfiguration(\n",
        "      jiant_task_container_config_path=f\"../tasks/run_configs/{task_name}_run_config.json\",\n",
        "      output_dir=f\"../tasks/runs/{task_name}\",\n",
        "      hf_pretrained_model_name_or_path=model_name,\n",
        "      model_path=f\"../tasks/models/{model_name}/model/model.p\",\n",
        "      model_config_path=f\"../tasks/models/{model_name}/model/config.json\",\n",
        "      learning_rate=1e-3,\n",
        "      eval_every_steps=1000,\n",
        "      do_train=True,\n",
        "      do_val=True,\n",
        "      do_save=True,\n",
        "      force_overwrite=True,\n",
        "      num_hidden_layers=num_layers,\n",
        "  )\n",
        "  return main_runscript.run_loop(run_args)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "num_layers = list(range(1, 13, 2))      # from 1 to 12 layers\n",
        "\n",
        "def plot_task(model, task, linestyle):\n",
        "  y = [float(results[model][task][n_layers]['f1_macro']) for n_layers in num_layers]\n",
        "  plt.plot(num_layers, y,linestyle, label=model)\n",
        "  plt.legend()\n",
        "  plt.suptitle(task)\n",
        "\n",
        "model_to_linestyle = {\n",
        "    \"bert-base-uncased\": \":g\", \n",
        "    \"csarron/bert-base-uncased-squad-v1\": \"-y\",\n",
        "\n",
        "}\n",
        "\n",
        "os.makedirs(f'../tasks/results', exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOMHEETaEhq-"
      },
      "source": [
        "result = \"model\\ttask\\tlayer\\tacc\\tf1_macro\\n\"\n",
        "results = {}\n",
        "\n",
        "for model in models:\n",
        "  results[model] = {}\n",
        "  for task in task_names:\n",
        "    results[model][task] = {}\n",
        "    for n_layers in num_layers:\n",
        "      results[model][task][n_layers] = {}\n",
        "      create_jiant_task_config(task)\n",
        "      probing_output = run_probing_task(task, model, n_layers)\n",
        "      acc = str(probing_output[task][\"metrics\"][\"minor\"][\"acc\"])\n",
        "      f1_macro = str(probing_output[task][\"metrics\"][\"minor\"][\"f1_macro\"])\n",
        "      result += model + \"\\t\" + task + \"\\t\" + str(n_layers) + \"\\t\" + acc + \"\\t\" + f1_macro + \"\\n\"\n",
        "      results[model][task][n_layers]['acc'] = acc\n",
        "      results[model][task][n_layers]['f1_macro'] = f1_macro\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wse2BUI9b-GV",
        "outputId": "264af5c8-0d5b-4566-8119-ce53d581d10d"
      },
      "source": [
        "print(result)\n",
        "plot_task('csarron/bert-base-uncased-squad-v1', 'sup-squad', '-y')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "model\ttask\tlayer\tacc\tf1_macro\n",
            "csarron/bert-base-uncased-squad-v1\tcoref\t1\t0.5476190476190477\t0.47527472527472525\n",
            "csarron/bert-base-uncased-squad-v1\tcoref\t2\t0.5476190476190477\t0.3548387096774194\n",
            "csarron/bert-base-uncased-squad-v1\tcoref\t3\t0.5714285714285714\t0.33333333333333337\n",
            "csarron/bert-base-uncased-squad-v1\tcoref\t4\t0.6904761904761905\t0.33333333333333337\n",
            "csarron/bert-base-uncased-squad-v1\tcoref\t5\t0.5\t0.0\n",
            "csarron/bert-base-uncased-squad-v1\tcoref\t6\t0.8571428571428571\t0.4615384615384615\n",
            "csarron/bert-base-uncased-squad-v1\tcoref\t7\t0.5\t0.0\n",
            "csarron/bert-base-uncased-squad-v1\tcoref\t8\t0.8571428571428571\t0.4615384615384615\n",
            "csarron/bert-base-uncased-squad-v1\tcoref\t9\t0.8571428571428571\t0.4615384615384615\n",
            "csarron/bert-base-uncased-squad-v1\tcoref\t10\t0.8571428571428571\t0.4615384615384615\n",
            "csarron/bert-base-uncased-squad-v1\tcoref\t11\t0.8571428571428571\t0.4615384615384615\n",
            "csarron/bert-base-uncased-squad-v1\tcoref\t12\t0.8571428571428571\t0.4615384615384615\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Qsiv7OQkLNT",
        "tags": [
          "outputPrepend"
        ]
      },
      "source": [
        "# Probe NER task with bert-base and bert-squad and plot macro f1 score\n",
        "import json\n",
        "\n",
        "result = \"model\\ttask\\tlayer\\tacc\\tf1_macro\\n\"\n",
        "results = {}\n",
        "task = \"ner\"\n",
        "\n",
        "for model in models:\n",
        "  results[model] = {}\n",
        "  results[model][task] = {}\n",
        "  for n_layers in num_layers:\n",
        "    results[model][task][n_layers] = {}\n",
        "    create_jiant_task_config(task)\n",
        "    probing_output = run_probing_task(task, model, n_layers)\n",
        "    acc = str(probing_output[task][\"metrics\"][\"minor\"][\"acc\"])\n",
        "    f1_macro = str(probing_output[task][\"metrics\"][\"minor\"][\"f1_macro\"])\n",
        "    result += model + \"\\t\" + task + \"\\t\" + str(n_layers) + \"\\t\" + acc + \"\\t\" + f1_macro + \"\\n\"\n",
        "    results[model][task][n_layers]['acc'] = acc\n",
        "    results[model][task][n_layers]['f1_macro'] = f1_macro\n",
        "\n",
        "with open(f'../tasks/results/{task}.json', 'w') as f:\n",
        "  json.dump(results, f)\n",
        "\n",
        "for model in models:\n",
        "  plot_task(model, task, model_to_linestyle[model])\n",
        "plt.show()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "l/model.p\n",
            "  model_config_path: ../models/bert-base-uncased/model/config.json\n",
            "  model_load_mode: from_transformers\n",
            "  num_hidden_layers: 3\n",
            "  do_train: True\n",
            "  do_val: True\n",
            "  do_save: True\n",
            "  do_save_last: False\n",
            "  do_save_best: False\n",
            "  write_val_preds: False\n",
            "  write_test_preds: False\n",
            "  eval_every_steps: 1000\n",
            "  save_every_steps: 0\n",
            "  save_checkpoint_every_steps: 0\n",
            "  no_improvements_for_n_evals: 0\n",
            "  keep_checkpoint_when_done: False\n",
            "  force_overwrite: True\n",
            "  seed: -1\n",
            "  learning_rate: 0.001\n",
            "  adam_epsilon: 1e-08\n",
            "  max_grad_norm: 1.0\n",
            "  optimizer_type: adam\n",
            "  no_cuda: False\n",
            "  fp16: False\n",
            "  fp16_opt_level: O1\n",
            "  local_rank: -1\n",
            "  server_ip: \n",
            "  server_port: \n",
            "device: cuda n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "Using seed: 3828254525\n",
            "{\n",
            "  \"jiant_task_container_config_path\": \"../run_configs/ner_run_config.json\",\n",
            "  \"output_dir\": \"../runs/ner\",\n",
            "  \"hf_pretrained_model_name_or_path\": \"bert-base-uncased\",\n",
            "  \"model_path\": \"../models/bert-base-uncased/model/model.p\",\n",
            "  \"model_config_path\": \"../models/bert-base-uncased/model/config.json\",\n",
            "  \"model_load_mode\": \"from_transformers\",\n",
            "  \"num_hidden_layers\": 3,\n",
            "  \"do_train\": true,\n",
            "  \"do_val\": true,\n",
            "  \"do_save\": true,\n",
            "  \"do_save_last\": false,\n",
            "  \"do_save_best\": false,\n",
            "  \"write_val_preds\": false,\n",
            "  \"write_test_preds\": false,\n",
            "  \"eval_every_steps\": 1000,\n",
            "  \"save_every_steps\": 0,\n",
            "  \"save_checkpoint_every_steps\": 0,\n",
            "  \"no_improvements_for_n_evals\": 0,\n",
            "  \"keep_checkpoint_when_done\": false,\n",
            "  \"force_overwrite\": true,\n",
            "  \"seed\": 3828254525,\n",
            "  \"learning_rate\": 0.001,\n",
            "  \"adam_epsilon\": 1e-08,\n",
            "  \"max_grad_norm\": 1.0,\n",
            "  \"optimizer_type\": \"adam\",\n",
            "  \"no_cuda\": false,\n",
            "  \"fp16\": false,\n",
            "  \"fp16_opt_level\": \"O1\",\n",
            "  \"local_rank\": -1,\n",
            "  \"server_ip\": \"\",\n",
            "  \"server_port\": \"\"\n",
            "}\n",
            "1\n",
            "Creating Tasks:\n",
            "    ner (NerTask): ../configs/ner_config.json\n",
            "Training:   0%|          | 0/3910 [00:00<?, ?it/s]No optimizer decay for:\n",
            "  encoder.embeddings.LayerNorm.weight\n",
            "  encoder.embeddings.LayerNorm.bias\n",
            "  encoder.encoder.layer.0.attention.self.query.bias\n",
            "  encoder.encoder.layer.0.attention.self.key.bias\n",
            "  encoder.encoder.layer.0.attention.self.value.bias\n",
            "  encoder.encoder.layer.0.attention.output.dense.bias\n",
            "  encoder.encoder.layer.0.attention.output.LayerNorm.weight\n",
            "  encoder.encoder.layer.0.attention.output.LayerNorm.bias\n",
            "  encoder.encoder.layer.0.intermediate.dense.bias\n",
            "  encoder.encoder.layer.0.output.dense.bias\n",
            "  encoder.encoder.layer.0.output.LayerNorm.weight\n",
            "  encoder.encoder.layer.0.output.LayerNorm.bias\n",
            "  encoder.encoder.layer.1.attention.self.query.bias\n",
            "  encoder.encoder.layer.1.attention.self.key.bias\n",
            "  encoder.encoder.layer.1.attention.self.value.bias\n",
            "  encoder.encoder.layer.1.attention.output.dense.bias\n",
            "  encoder.encoder.layer.1.attention.output.LayerNorm.weight\n",
            "  encoder.encoder.layer.1.attention.output.LayerNorm.bias\n",
            "  encoder.encoder.layer.1.intermediate.dense.bias\n",
            "  encoder.encoder.layer.1.output.dense.bias\n",
            "  encoder.encoder.layer.1.output.LayerNorm.weight\n",
            "  encoder.encoder.layer.1.output.LayerNorm.bias\n",
            "  encoder.encoder.layer.2.attention.self.query.bias\n",
            "  encoder.encoder.layer.2.attention.self.key.bias\n",
            "  encoder.encoder.layer.2.attention.self.value.bias\n",
            "  encoder.encoder.layer.2.attention.output.dense.bias\n",
            "  encoder.encoder.layer.2.attention.output.LayerNorm.weight\n",
            "  encoder.encoder.layer.2.attention.output.LayerNorm.bias\n",
            "  encoder.encoder.layer.2.intermediate.dense.bias\n",
            "  encoder.encoder.layer.2.output.dense.bias\n",
            "  encoder.encoder.layer.2.output.LayerNorm.weight\n",
            "  encoder.encoder.layer.2.output.LayerNorm.bias\n",
            "  encoder.pooler.dense.bias\n",
            "  taskmodels_dict.ner.head.span_attention_extractor._global_attention._module.bias\n",
            "  taskmodels_dict.ner.head.classifier.bias\n",
            "Using AdamW\n",
            "Training:  26%|██▌       | 998/3910 [03:49<11:33,  4.20it/s]\n",
            "Eval (ner, Val):   0%|          | 0/250 [00:00<?, ?it/s]\u001b[A\n",
            "Eval (ner, Val):   1%|          | 2/250 [00:00<00:13, 17.81it/s]\u001b[A\n",
            "Eval (ner, Val):   3%|▎         | 7/250 [00:00<00:11, 21.71it/s]\u001b[A\n",
            "Eval (ner, Val):   5%|▍         | 12/250 [00:00<00:09, 25.65it/s]\u001b[A\n",
            "Eval (ner, Val):   7%|▋         | 17/250 [00:00<00:07, 29.70it/s]\u001b[A\n",
            "Eval (ner, Val):   9%|▉         | 22/250 [00:00<00:06, 33.02it/s]\u001b[A\n",
            "Eval (ner, Val):  11%|█         | 27/250 [00:00<00:06, 35.72it/s]\u001b[A\n",
            "Eval (ner, Val):  13%|█▎        | 32/250 [00:00<00:05, 37.89it/s]\u001b[A\n",
            "Eval (ner, Val):  15%|█▍        | 37/250 [00:00<00:05, 39.74it/s]\u001b[A\n",
            "Eval (ner, Val):  17%|█▋        | 42/250 [00:01<00:05, 41.06it/s]\u001b[A\n",
            "Eval (ner, Val):  19%|█▉        | 47/250 [00:01<00:04, 42.07it/s]\u001b[A\n",
            "Eval (ner, Val):  21%|██        | 52/250 [00:01<00:04, 42.77it/s]\u001b[A\n",
            "Eval (ner, Val):  23%|██▎       | 57/250 [00:01<00:04, 43.34it/s]\u001b[A\n",
            "Eval (ner, Val):  25%|██▍       | 62/250 [00:01<00:04, 44.00it/s]\u001b[A\n",
            "Eval (ner, Val):  27%|██▋       | 67/250 [00:01<00:04, 43.78it/s]\u001b[A\n",
            "Eval (ner, Val):  29%|██▉       | 72/250 [00:01<00:04, 43.92it/s]\u001b[A\n",
            "Eval (ner, Val):  31%|███       | 77/250 [00:01<00:03, 43.87it/s]\u001b[A\n",
            "Eval (ner, Val):  33%|███▎      | 82/250 [00:01<00:03, 44.41it/s]\u001b[A\n",
            "Eval (ner, Val):  35%|███▍      | 87/250 [00:02<00:03, 44.16it/s]\u001b[A\n",
            "Eval (ner, Val):  37%|███▋      | 92/250 [00:02<00:03, 44.31it/s]\u001b[A\n",
            "Eval (ner, Val):  39%|███▉      | 97/250 [00:02<00:03, 44.29it/s]\u001b[A\n",
            "Eval (ner, Val):  41%|████      | 102/250 [00:02<00:03, 44.68it/s]\u001b[A\n",
            "Eval (ner, Val):  43%|████▎     | 107/250 [00:02<00:03, 44.72it/s]\u001b[A\n",
            "Eval (ner, Val):  45%|████▍     | 112/250 [00:02<00:03, 44.38it/s]\u001b[A\n",
            "Eval (ner, Val):  47%|████▋     | 117/250 [00:02<00:03, 44.21it/s]\u001b[A\n",
            "Eval (ner, Val):  49%|████▉     | 122/250 [00:02<00:02, 44.81it/s]\u001b[A\n",
            "Eval (ner, Val):  51%|█████     | 127/250 [00:02<00:02, 45.05it/s]\u001b[A\n",
            "Eval (ner, Val):  53%|█████▎    | 132/250 [00:03<00:02, 44.93it/s]\u001b[A\n",
            "Eval (ner, Val):  55%|█████▍    | 137/250 [00:03<00:02, 45.22it/s]\u001b[A\n",
            "Eval (ner, Val):  57%|█████▋    | 142/250 [00:03<00:02, 45.46it/s]\u001b[A\n",
            "Eval (ner, Val):  59%|█████▉    | 147/250 [00:03<00:02, 45.37it/s]\u001b[A\n",
            "Eval (ner, Val):  61%|██████    | 152/250 [00:03<00:02, 45.01it/s]\u001b[A\n",
            "Eval (ner, Val):  63%|██████▎   | 157/250 [00:03<00:02, 44.58it/s]\u001b[A\n",
            "Eval (ner, Val):  65%|██████▍   | 162/250 [00:03<00:01, 44.53it/s]\u001b[A\n",
            "Eval (ner, Val):  67%|██████▋   | 167/250 [00:03<00:01, 44.61it/s]\u001b[A\n",
            "Eval (ner, Val):  69%|██████▉   | 172/250 [00:03<00:01, 44.31it/s]\u001b[A\n",
            "Eval (ner, Val):  71%|███████   | 177/250 [00:04<00:01, 44.36it/s]\u001b[A\n",
            "Eval (ner, Val):  73%|███████▎  | 182/250 [00:04<00:01, 44.57it/s]\u001b[A\n",
            "Eval (ner, Val):  75%|███████▍  | 187/250 [00:04<00:01, 44.98it/s]\u001b[A\n",
            "Eval (ner, Val):  77%|███████▋  | 192/250 [00:04<00:01, 45.34it/s]\u001b[A\n",
            "Eval (ner, Val):  79%|███████▉  | 197/250 [00:04<00:01, 45.35it/s]\u001b[A\n",
            "Eval (ner, Val):  81%|████████  | 202/250 [00:04<00:01, 44.99it/s]\u001b[A\n",
            "Eval (ner, Val):  83%|████████▎ | 207/250 [00:04<00:00, 44.94it/s]\u001b[A\n",
            "Eval (ner, Val):  85%|████████▍ | 212/250 [00:04<00:00, 45.00it/s]\u001b[A\n",
            "Eval (ner, Val):  87%|████████▋ | 217/250 [00:04<00:00, 45.52it/s]\u001b[A\n",
            "Eval (ner, Val):  89%|████████▉ | 222/250 [00:05<00:00, 45.98it/s]\u001b[A\n",
            "Eval (ner, Val):  91%|█████████ | 227/250 [00:05<00:00, 45.65it/s]\u001b[A\n",
            "Eval (ner, Val):  93%|█████████▎| 232/250 [00:05<00:00, 45.32it/s]\u001b[A\n",
            "Eval (ner, Val):  95%|█████████▍| 237/250 [00:05<00:00, 45.56it/s]\u001b[A\n",
            "Eval (ner, Val):  97%|█████████▋| 242/250 [00:05<00:00, 46.14it/s]\u001b[A\n",
            "Eval (ner, Val): 100%|██████████| 250/250 [00:05<00:00, 44.42it/s]\n",
            "Training:  51%|█████     | 1998/3910 [07:43<07:16,  4.38it/s]\n",
            "Eval (ner, Val):   0%|          | 0/250 [00:00<?, ?it/s]\u001b[A\n",
            "Eval (ner, Val):   1%|          | 3/250 [00:00<00:09, 25.75it/s]\u001b[A\n",
            "Eval (ner, Val):   3%|▎         | 8/250 [00:00<00:08, 29.74it/s]\u001b[A\n",
            "Eval (ner, Val):   5%|▌         | 13/250 [00:00<00:07, 33.41it/s]\u001b[A\n",
            "Eval (ner, Val):   7%|▋         | 18/250 [00:00<00:06, 36.00it/s]\u001b[A\n",
            "Eval (ner, Val):   9%|▉         | 23/250 [00:00<00:05, 38.19it/s]\u001b[A\n",
            "Eval (ner, Val):  11%|█         | 28/250 [00:00<00:05, 40.14it/s]\u001b[A\n",
            "Eval (ner, Val):  13%|█▎        | 33/250 [00:00<00:05, 41.83it/s]\u001b[A\n",
            "Eval (ner, Val):  15%|█▌        | 38/250 [00:00<00:04, 42.97it/s]\u001b[A\n",
            "Eval (ner, Val):  17%|█▋        | 43/250 [00:00<00:04, 43.11it/s]\u001b[A\n",
            "Eval (ner, Val):  19%|█▉        | 48/250 [00:01<00:04, 43.66it/s]\u001b[A\n",
            "Eval (ner, Val):  21%|██        | 53/250 [00:01<00:04, 43.78it/s]\u001b[A\n",
            "Eval (ner, Val):  23%|██▎       | 58/250 [00:01<00:04, 43.87it/s]\u001b[A\n",
            "Eval (ner, Val):  25%|██▌       | 63/250 [00:01<00:04, 43.82it/s]\u001b[A\n",
            "Eval (ner, Val):  27%|██▋       | 68/250 [00:01<00:04, 43.90it/s]\u001b[A\n",
            "Eval (ner, Val):  29%|██▉       | 73/250 [00:01<00:04, 43.86it/s]\u001b[A\n",
            "Eval (ner, Val):  31%|███       | 78/250 [00:01<00:03, 44.13it/s]\u001b[A\n",
            "Eval (ner, Val):  33%|███▎      | 83/250 [00:01<00:03, 44.54it/s]\u001b[A\n",
            "Eval (ner, Val):  35%|███▌      | 88/250 [00:02<00:03, 45.07it/s]\u001b[A\n",
            "Eval (ner, Val):  37%|███▋      | 93/250 [00:02<00:03, 45.72it/s]\u001b[A\n",
            "Eval (ner, Val):  39%|███▉      | 98/250 [00:02<00:03, 46.30it/s]\u001b[A\n",
            "Eval (ner, Val):  41%|████      | 103/250 [00:02<00:03, 46.38it/s]\u001b[A\n",
            "Eval (ner, Val):  43%|████▎     | 108/250 [00:02<00:03, 46.75it/s]\u001b[A\n",
            "Eval (ner, Val):  45%|████▌     | 113/250 [00:02<00:02, 46.85it/s]\u001b[A\n",
            "Eval (ner, Val):  47%|████▋     | 118/250 [00:02<00:02, 47.11it/s]\u001b[A\n",
            "Eval (ner, Val):  49%|████▉     | 123/250 [00:02<00:02, 47.29it/s]\u001b[A\n",
            "Eval (ner, Val):  51%|█████     | 128/250 [00:02<00:02, 47.36it/s]\u001b[A\n",
            "Eval (ner, Val):  53%|█████▎    | 133/250 [00:02<00:02, 47.10it/s]\u001b[A\n",
            "Eval (ner, Val):  55%|█████▌    | 138/250 [00:03<00:02, 47.11it/s]\u001b[A\n",
            "Eval (ner, Val):  57%|█████▋    | 143/250 [00:03<00:02, 47.19it/s]\u001b[A\n",
            "Eval (ner, Val):  59%|█████▉    | 148/250 [00:03<00:02, 47.38it/s]\u001b[A\n",
            "Eval (ner, Val):  61%|██████    | 153/250 [00:03<00:02, 47.32it/s]\u001b[A\n",
            "Eval (ner, Val):  63%|██████▎   | 158/250 [00:03<00:01, 47.42it/s]\u001b[A\n",
            "Eval (ner, Val):  65%|██████▌   | 163/250 [00:03<00:01, 47.34it/s]\u001b[A\n",
            "Eval (ner, Val):  67%|██████▋   | 168/250 [00:03<00:01, 47.35it/s]\u001b[A\n",
            "Eval (ner, Val):  69%|██████▉   | 173/250 [00:03<00:01, 47.24it/s]\u001b[A\n",
            "Eval (ner, Val):  71%|███████   | 178/250 [00:03<00:01, 47.22it/s]\u001b[A\n",
            "Eval (ner, Val):  73%|███████▎  | 183/250 [00:04<00:01, 47.41it/s]\u001b[A\n",
            "Eval (ner, Val):  75%|███████▌  | 188/250 [00:04<00:01, 47.50it/s]\u001b[A\n",
            "Eval (ner, Val):  77%|███████▋  | 193/250 [00:04<00:01, 47.46it/s]\u001b[A\n",
            "Eval (ner, Val):  79%|███████▉  | 198/250 [00:04<00:01, 47.38it/s]\u001b[A\n",
            "Eval (ner, Val):  81%|████████  | 203/250 [00:04<00:00, 47.44it/s]\u001b[A\n",
            "Eval (ner, Val):  83%|████████▎ | 208/250 [00:04<00:00, 47.42it/s]\u001b[A\n",
            "Eval (ner, Val):  85%|████████▌ | 213/250 [00:04<00:00, 47.38it/s]\u001b[A\n",
            "Eval (ner, Val):  87%|████████▋ | 218/250 [00:04<00:00, 47.47it/s]\u001b[A\n",
            "Eval (ner, Val):  89%|████████▉ | 223/250 [00:04<00:00, 47.14it/s]\u001b[A\n",
            "Eval (ner, Val):  91%|█████████ | 228/250 [00:04<00:00, 47.23it/s]\u001b[A\n",
            "Eval (ner, Val):  93%|█████████▎| 233/250 [00:05<00:00, 47.32it/s]\u001b[A\n",
            "Eval (ner, Val):  95%|█████████▌| 238/250 [00:05<00:00, 47.46it/s]\u001b[A\n",
            "Eval (ner, Val):  97%|█████████▋| 243/250 [00:05<00:00, 47.48it/s]\u001b[A\n",
            "Eval (ner, Val): 100%|██████████| 250/250 [00:05<00:00, 46.04it/s]\n",
            "Training:  77%|███████▋  | 2998/3910 [11:41<03:34,  4.25it/s]\n",
            "Eval (ner, Val):   0%|          | 0/250 [00:00<?, ?it/s]\u001b[A\n",
            "Eval (ner, Val):   1%|          | 2/250 [00:00<00:12, 19.23it/s]\u001b[A\n",
            "Eval (ner, Val):   3%|▎         | 7/250 [00:00<00:10, 23.14it/s]\u001b[A\n",
            "Eval (ner, Val):   5%|▍         | 12/250 [00:00<00:08, 27.06it/s]\u001b[A\n",
            "Eval (ner, Val):   7%|▋         | 17/250 [00:00<00:07, 30.70it/s]\u001b[A\n",
            "Eval (ner, Val):   9%|▉         | 22/250 [00:00<00:06, 33.76it/s]\u001b[A\n",
            "Eval (ner, Val):  11%|█         | 27/250 [00:00<00:06, 36.43it/s]\u001b[A\n",
            "Eval (ner, Val):  13%|█▎        | 32/250 [00:00<00:05, 38.55it/s]\u001b[A\n",
            "Eval (ner, Val):  15%|█▍        | 37/250 [00:00<00:05, 40.11it/s]\u001b[A\n",
            "Eval (ner, Val):  17%|█▋        | 42/250 [00:01<00:05, 41.33it/s]\u001b[A\n",
            "Eval (ner, Val):  19%|█▉        | 47/250 [00:01<00:04, 42.03it/s]\u001b[A\n",
            "Eval (ner, Val):  21%|██        | 52/250 [00:01<00:04, 42.97it/s]\u001b[A\n",
            "Eval (ner, Val):  23%|██▎       | 57/250 [00:01<00:04, 43.71it/s]\u001b[A\n",
            "Eval (ner, Val):  25%|██▍       | 62/250 [00:01<00:04, 44.14it/s]\u001b[A\n",
            "Eval (ner, Val):  27%|██▋       | 67/250 [00:01<00:04, 44.03it/s]\u001b[A\n",
            "Eval (ner, Val):  29%|██▉       | 72/250 [00:01<00:04, 43.67it/s]\u001b[A\n",
            "Eval (ner, Val):  31%|███       | 77/250 [00:01<00:03, 43.44it/s]\u001b[A\n",
            "Eval (ner, Val):  33%|███▎      | 82/250 [00:01<00:03, 43.42it/s]\u001b[A\n",
            "Eval (ner, Val):  35%|███▍      | 87/250 [00:02<00:03, 43.78it/s]\u001b[A\n",
            "Eval (ner, Val):  37%|███▋      | 92/250 [00:02<00:03, 43.45it/s]\u001b[A\n",
            "Eval (ner, Val):  39%|███▉      | 97/250 [00:02<00:03, 42.93it/s]\u001b[A\n",
            "Eval (ner, Val):  41%|████      | 102/250 [00:02<00:03, 42.92it/s]\u001b[A\n",
            "Eval (ner, Val):  43%|████▎     | 107/250 [00:02<00:03, 43.27it/s]\u001b[A\n",
            "Eval (ner, Val):  45%|████▍     | 112/250 [00:02<00:03, 43.04it/s]\u001b[A\n",
            "Eval (ner, Val):  47%|████▋     | 117/250 [00:02<00:03, 42.55it/s]\u001b[A\n",
            "Eval (ner, Val):  49%|████▉     | 122/250 [00:02<00:02, 43.04it/s]\u001b[A\n",
            "Eval (ner, Val):  51%|█████     | 127/250 [00:02<00:02, 42.67it/s]\u001b[A\n",
            "Eval (ner, Val):  53%|█████▎    | 132/250 [00:03<00:02, 42.57it/s]\u001b[A\n",
            "Eval (ner, Val):  55%|█████▍    | 137/250 [00:03<00:02, 42.69it/s]\u001b[A\n",
            "Eval (ner, Val):  57%|█████▋    | 142/250 [00:03<00:02, 42.36it/s]\u001b[A\n",
            "Eval (ner, Val):  59%|█████▉    | 147/250 [00:03<00:02, 42.77it/s]\u001b[A\n",
            "Eval (ner, Val):  61%|██████    | 152/250 [00:03<00:02, 43.16it/s]\u001b[A\n",
            "Eval (ner, Val):  63%|██████▎   | 157/250 [00:03<00:02, 43.08it/s]\u001b[A\n",
            "Eval (ner, Val):  65%|██████▍   | 162/250 [00:03<00:02, 42.91it/s]\u001b[A\n",
            "Eval (ner, Val):  67%|██████▋   | 167/250 [00:03<00:01, 42.95it/s]\u001b[A\n",
            "Eval (ner, Val):  69%|██████▉   | 172/250 [00:04<00:01, 43.25it/s]\u001b[A\n",
            "Eval (ner, Val):  71%|███████   | 177/250 [00:04<00:01, 43.19it/s]\u001b[A\n",
            "Eval (ner, Val):  73%|███████▎  | 182/250 [00:04<00:01, 43.11it/s]\u001b[A\n",
            "Eval (ner, Val):  75%|███████▍  | 187/250 [00:04<00:01, 43.16it/s]\u001b[A\n",
            "Eval (ner, Val):  77%|███████▋  | 192/250 [00:04<00:01, 43.40it/s]\u001b[A\n",
            "Eval (ner, Val):  79%|███████▉  | 197/250 [00:04<00:01, 43.28it/s]\u001b[A\n",
            "Eval (ner, Val):  81%|████████  | 202/250 [00:04<00:01, 43.13it/s]\u001b[A\n",
            "Eval (ner, Val):  83%|████████▎ | 207/250 [00:04<00:00, 43.27it/s]\u001b[A\n",
            "Eval (ner, Val):  85%|████████▍ | 212/250 [00:04<00:00, 42.74it/s]\u001b[A\n",
            "Eval (ner, Val):  87%|████████▋ | 217/250 [00:05<00:00, 42.61it/s]\u001b[A\n",
            "Eval (ner, Val):  89%|████████▉ | 222/250 [00:05<00:00, 43.19it/s]\u001b[A\n",
            "Eval (ner, Val):  91%|█████████ | 227/250 [00:05<00:00, 43.56it/s]\u001b[A\n",
            "Eval (ner, Val):  93%|█████████▎| 232/250 [00:05<00:00, 43.36it/s]\u001b[A\n",
            "Eval (ner, Val):  95%|█████████▍| 237/250 [00:05<00:00, 43.39it/s]\u001b[A\n",
            "Eval (ner, Val):  97%|█████████▋| 242/250 [00:05<00:00, 43.88it/s]\u001b[A\n",
            "Eval (ner, Val): 100%|██████████| 250/250 [00:05<00:00, 43.04it/s]\n",
            "Training: 100%|█████████▉| 3909/3910 [15:21<00:00,  4.24it/s]\n",
            "Eval (ner, Val): 100%|██████████| 250/250 [00:05<00:00, 44.09it/s]\n",
            "Eval (ner, Val): 100%|██████████| 250/250 [00:05<00:00, 44.48it/s]\n",
            "Eval (ner, Val):   0%|          | 0/601 [00:00<?, ?it/s]Loading Best\n",
            "Eval (ner, Val): 100%|██████████| 601/601 [00:13<00:00, 45.39it/s]\n",
            "{\n",
            "  \"aggregated\": 0.4722222222222222,\n",
            "  \"ner\": {\n",
            "    \"loss\": 0.198882316268621,\n",
            "    \"metrics\": {\n",
            "      \"major\": 0.4722222222222222,\n",
            "      \"minor\": {\n",
            "        \"acc\": 0.9444444444444444,\n",
            "        \"f1_macro\": 0.0,\n",
            "        \"acc_and_f1_macro\": 0.4722222222222222\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "  jiant_task_container_config_path: ../run_configs/ner_run_config.json\n",
            "  output_dir: ../runs/ner\n",
            "  hf_pretrained_model_name_or_path: bert-base-uncased\n",
            "  model_path: ../models/bert-base-uncased/model/model.p\n",
            "  model_config_path: ../models/bert-base-uncased/model/config.json\n",
            "  model_load_mode: from_transformers\n",
            "  num_hidden_layers: 5\n",
            "  do_train: True\n",
            "  do_val: True\n",
            "  do_save: True\n",
            "  do_save_last: False\n",
            "  do_save_best: False\n",
            "  write_val_preds: False\n",
            "  write_test_preds: False\n",
            "  eval_every_steps: 1000\n",
            "  save_every_steps: 0\n",
            "  save_checkpoint_every_steps: 0\n",
            "  no_improvements_for_n_evals: 0\n",
            "  keep_checkpoint_when_done: False\n",
            "  force_overwrite: True\n",
            "  seed: -1\n",
            "  learning_rate: 0.001\n",
            "  adam_epsilon: 1e-08\n",
            "  max_grad_norm: 1.0\n",
            "  optimizer_type: adam\n",
            "  no_cuda: False\n",
            "  fp16: False\n",
            "  fp16_opt_level: O1\n",
            "  local_rank: -1\n",
            "  server_ip: \n",
            "  server_port: \n",
            "device: cuda n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "Using seed: 1063147142\n",
            "{\n",
            "  \"jiant_task_container_config_path\": \"../run_configs/ner_run_config.json\",\n",
            "  \"output_dir\": \"../runs/ner\",\n",
            "  \"hf_pretrained_model_name_or_path\": \"bert-base-uncased\",\n",
            "  \"model_path\": \"../models/bert-base-uncased/model/model.p\",\n",
            "  \"model_config_path\": \"../models/bert-base-uncased/model/config.json\",\n",
            "  \"model_load_mode\": \"from_transformers\",\n",
            "  \"num_hidden_layers\": 5,\n",
            "  \"do_train\": true,\n",
            "  \"do_val\": true,\n",
            "  \"do_save\": true,\n",
            "  \"do_save_last\": false,\n",
            "  \"do_save_best\": false,\n",
            "  \"write_val_preds\": false,\n",
            "  \"write_test_preds\": false,\n",
            "  \"eval_every_steps\": 1000,\n",
            "  \"save_every_steps\": 0,\n",
            "  \"save_checkpoint_every_steps\": 0,\n",
            "  \"no_improvements_for_n_evals\": 0,\n",
            "  \"keep_checkpoint_when_done\": false,\n",
            "  \"force_overwrite\": true,\n",
            "  \"seed\": 1063147142,\n",
            "  \"learning_rate\": 0.001,\n",
            "  \"adam_epsilon\": 1e-08,\n",
            "  \"max_grad_norm\": 1.0,\n",
            "  \"optimizer_type\": \"adam\",\n",
            "  \"no_cuda\": false,\n",
            "  \"fp16\": false,\n",
            "  \"fp16_opt_level\": \"O1\",\n",
            "  \"local_rank\": -1,\n",
            "  \"server_ip\": \"\",\n",
            "  \"server_port\": \"\"\n",
            "}\n",
            "1\n",
            "Creating Tasks:\n",
            "    ner (NerTask): ../configs/ner_config.json\n",
            "Training:   0%|          | 0/3910 [00:00<?, ?it/s]No optimizer decay for:\n",
            "  encoder.embeddings.LayerNorm.weight\n",
            "  encoder.embeddings.LayerNorm.bias\n",
            "  encoder.encoder.layer.0.attention.self.query.bias\n",
            "  encoder.encoder.layer.0.attention.self.key.bias\n",
            "  encoder.encoder.layer.0.attention.self.value.bias\n",
            "  encoder.encoder.layer.0.attention.output.dense.bias\n",
            "  encoder.encoder.layer.0.attention.output.LayerNorm.weight\n",
            "  encoder.encoder.layer.0.attention.output.LayerNorm.bias\n",
            "  encoder.encoder.layer.0.intermediate.dense.bias\n",
            "  encoder.encoder.layer.0.output.dense.bias\n",
            "  encoder.encoder.layer.0.output.LayerNorm.weight\n",
            "  encoder.encoder.layer.0.output.LayerNorm.bias\n",
            "  encoder.encoder.layer.1.attention.self.query.bias\n",
            "  encoder.encoder.layer.1.attention.self.key.bias\n",
            "  encoder.encoder.layer.1.attention.self.value.bias\n",
            "  encoder.encoder.layer.1.attention.output.dense.bias\n",
            "  encoder.encoder.layer.1.attention.output.LayerNorm.weight\n",
            "  encoder.encoder.layer.1.attention.output.LayerNorm.bias\n",
            "  encoder.encoder.layer.1.intermediate.dense.bias\n",
            "  encoder.encoder.layer.1.output.dense.bias\n",
            "  encoder.encoder.layer.1.output.LayerNorm.weight\n",
            "  encoder.encoder.layer.1.output.LayerNorm.bias\n",
            "  encoder.encoder.layer.2.attention.self.query.bias\n",
            "  encoder.encoder.layer.2.attention.self.key.bias\n",
            "  encoder.encoder.layer.2.attention.self.value.bias\n",
            "  encoder.encoder.layer.2.attention.output.dense.bias\n",
            "  encoder.encoder.layer.2.attention.output.LayerNorm.weight\n",
            "  encoder.encoder.layer.2.attention.output.LayerNorm.bias\n",
            "  encoder.encoder.layer.2.intermediate.dense.bias\n",
            "  encoder.encoder.layer.2.output.dense.bias\n",
            "  encoder.encoder.layer.2.output.LayerNorm.weight\n",
            "  encoder.encoder.layer.2.output.LayerNorm.bias\n",
            "  encoder.encoder.layer.3.attention.self.query.bias\n",
            "  encoder.encoder.layer.3.attention.self.key.bias\n",
            "  encoder.encoder.layer.3.attention.self.value.bias\n",
            "  encoder.encoder.layer.3.attention.output.dense.bias\n",
            "  encoder.encoder.layer.3.attention.output.LayerNorm.weight\n",
            "  encoder.encoder.layer.3.attention.output.LayerNorm.bias\n",
            "  encoder.encoder.layer.3.intermediate.dense.bias\n",
            "  encoder.encoder.layer.3.output.dense.bias\n",
            "  encoder.encoder.layer.3.output.LayerNorm.weight\n",
            "  encoder.encoder.layer.3.output.LayerNorm.bias\n",
            "  encoder.encoder.layer.4.attention.self.query.bias\n",
            "  encoder.encoder.layer.4.attention.self.key.bias\n",
            "  encoder.encoder.layer.4.attention.self.value.bias\n",
            "  encoder.encoder.layer.4.attention.output.dense.bias\n",
            "  encoder.encoder.layer.4.attention.output.LayerNorm.weight\n",
            "  encoder.encoder.layer.4.attention.output.LayerNorm.bias\n",
            "  encoder.encoder.layer.4.intermediate.dense.bias\n",
            "  encoder.encoder.layer.4.output.dense.bias\n",
            "  encoder.encoder.layer.4.output.LayerNorm.weight\n",
            "  encoder.encoder.layer.4.output.LayerNorm.bias\n",
            "  encoder.pooler.dense.bias\n",
            "  taskmodels_dict.ner.head.span_attention_extractor._global_attention._module.bias\n",
            "  taskmodels_dict.ner.head.classifier.bias\n",
            "Using AdamW\n",
            "Training:   0%|          | 1/3910 [00:01<1:08:43,  1.05s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "CUDA out of memory. Tried to allocate 54.00 MiB (GPU 0; 3.82 GiB total capacity; 2.22 GiB already allocated; 75.75 MiB free; 2.26 GiB reserved in total by PyTorch)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_9353/3346561864.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn_layers\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mcreate_jiant_task_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mprobing_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_probing_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobing_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"metrics\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"minor\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"acc\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mf1_macro\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobing_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"metrics\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"minor\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"f1_macro\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipykernel_9353/3897969010.py\u001b[0m in \u001b[0;36mrun_probing_task\u001b[0;34m(task_name, model_name, num_layers)\u001b[0m\n\u001b[1;32m     18\u001b[0m       \u001b[0mnum_hidden_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m   )\n\u001b[0;32m---> 20\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mmain_runscript\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m~/Documents/Deep Learning/How-Does-Bert-Answer-QA-DLP2021/src/probing-tasks/jiant/jiant/proj/main/runscript.py\u001b[0m in \u001b[0;36mrun_loop\u001b[0;34m(args, checkpoint)\u001b[0m\n\u001b[1;32m    172\u001b[0m                 \u001b[0mmetarunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"metarunner_state\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"metarunner_state\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m             \u001b[0mmetarunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_train_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_val\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/Documents/Deep Learning/How-Does-Bert-Answer-QA-DLP2021/src/probing-tasks/jiant/jiant/shared/metarunner.py\u001b[0m in \u001b[0;36mrun_train_loop\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myield_train_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_save_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/Documents/Deep Learning/How-Does-Bert-Answer-QA-DLP2021/src/probing-tasks/jiant/jiant/proj/main/metarunner.py\u001b[0m in \u001b[0;36myield_train_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    107\u001b[0m                 \u001b[0mtrain_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             )\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mtrain_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minject_at_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/Documents/Deep Learning/How-Does-Bert-Answer-QA-DLP2021/src/probing-tasks/jiant/jiant/proj/main/runner.py\u001b[0m in \u001b[0;36mrun_train_context\u001b[0;34m(self, verbose)\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         ):\n\u001b[0;32m---> 75\u001b[0;31m             self.run_train_step(\n\u001b[0m\u001b[1;32m     76\u001b[0m                 \u001b[0mtrain_dataloader_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_dataloader_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             )\n",
            "\u001b[0;32m~/Documents/Deep Learning/How-Does-Bert-Answer-QA-DLP2021/src/probing-tasks/jiant/jiant/proj/main/runner.py\u001b[0m in \u001b[0;36mrun_train_step\u001b[0;34m(self, train_dataloader_dict, train_state)\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_metadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dataloader_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtask_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m             model_output = wrap_jiant_forward(\n\u001b[0m\u001b[1;32m    105\u001b[0m                 \u001b[0mjiant_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjiant_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             )\n",
            "\u001b[0;32m~/Documents/Deep Learning/How-Does-Bert-Answer-QA-DLP2021/src/probing-tasks/jiant/jiant/proj/main/modeling/primary.py\u001b[0m in \u001b[0;36mwrap_jiant_forward\u001b[0;34m(jiant_model, batch, task, compute_loss)\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0mis_multi_gpu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjiant_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataParallel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     model_output = construct_output_from_dict(\n\u001b[0;32m--> 107\u001b[0;31m         jiant_model(\n\u001b[0m\u001b[1;32m    108\u001b[0m             \u001b[0mbatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_multi_gpu\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         )\n",
            "\u001b[0;32m~/Documents/Deep Learning/.bert_venv/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/Documents/Deep Learning/How-Does-Bert-Answer-QA-DLP2021/src/probing-tasks/jiant/jiant/proj/main/modeling/primary.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, batch, task, compute_loss)\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mtaskmodel_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_to_taskmodel_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtask_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mtaskmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtaskmodels_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtaskmodel_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         return taskmodel(\n\u001b[0m\u001b[1;32m     78\u001b[0m             \u001b[0mbatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         ).to_dict()\n",
            "\u001b[0;32m~/Documents/Deep Learning/.bert_venv/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/Documents/Deep Learning/How-Does-Bert-Answer-QA-DLP2021/src/probing-tasks/jiant/jiant/proj/main/modeling/taskmodels.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, batch, tokenizer, compute_loss)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m         encoder_output = self.encoder.encode(\n\u001b[0m\u001b[1;32m    216\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegment_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msegment_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         )\n",
            "\u001b[0;32m~/Documents/Deep Learning/How-Does-Bert-Answer-QA-DLP2021/src/probing-tasks/jiant/jiant/proj/main/modeling/primary.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, input_ids, segment_ids, input_mask, output_hidden_states)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegment_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m         output = self.forward(\n\u001b[0m\u001b[1;32m    208\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m             \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msegment_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/Documents/Deep Learning/.bert_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    969\u001b[0m             \u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m         )\n\u001b[0;32m--> 971\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m    972\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    973\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/Documents/Deep Learning/.bert_venv/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/Documents/Deep Learning/.bert_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    566\u001b[0m                 )\n\u001b[1;32m    567\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    569\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/Documents/Deep Learning/.bert_venv/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/Documents/Deep Learning/.bert_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    454\u001b[0m         \u001b[0;31m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m         \u001b[0mself_attn_past_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 456\u001b[0;31m         self_attention_outputs = self.attention(\n\u001b[0m\u001b[1;32m    457\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/Documents/Deep Learning/.bert_venv/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/Documents/Deep Learning/.bert_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    385\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m     ):\n\u001b[0;32m--> 387\u001b[0;31m         self_outputs = self.self(\n\u001b[0m\u001b[1;32m    388\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/Documents/Deep Learning/.bert_venv/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/Documents/Deep Learning/.bert_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m         \u001b[0;31m# Take the dot product between \"query\" and \"key\" to get the raw attention scores.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m         \u001b[0mattention_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_embedding_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"relative_key\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_embedding_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"relative_key_query\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 54.00 MiB (GPU 0; 3.82 GiB total capacity; 2.22 GiB already allocated; 75.75 MiB free; 2.26 GiB reserved in total by PyTorch)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgma94TxknlJ"
      },
      "source": [
        "# Probe SEMEVAL task with bert-base and bert-squad and plot macro f1 score\n",
        "import json\n",
        "\n",
        "result = \"model\\ttask\\tlayer\\tacc\\tf1_macro\\n\"\n",
        "results = {}\n",
        "task = \"semeval\"\n",
        "\n",
        "for model in models:\n",
        "  results[model] = {}\n",
        "  results[model][task] = {}\n",
        "  for n_layers in num_layers:\n",
        "    results[model][task][n_layers] = {}\n",
        "    create_jiant_task_config(task)\n",
        "    probing_output = run_probing_task(task, model, n_layers)\n",
        "    acc = str(probing_output[task][\"metrics\"][\"minor\"][\"acc\"])\n",
        "    f1_macro = str(probing_output[task][\"metrics\"][\"minor\"][\"f1_macro\"])\n",
        "    result += model + \"\\t\" + task + \"\\t\" + str(n_layers) + \"\\t\" + acc + \"\\t\" + f1_macro + \"\\n\"\n",
        "    results[model][task][n_layers]['acc'] = acc\n",
        "    results[model][task][n_layers]['f1_macro'] = f1_macro\n",
        "\n",
        "with open(f'../tasks/results/{task}.json', 'w') as f:\n",
        "  json.dump(results, f)\n",
        "\n",
        "for model in models:\n",
        "  plot_task(model, task, model_to_linestyle[model])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ju9yPJx0ku4l"
      },
      "source": [
        "# Probe COREF task with bert-base and bert-squad and plot macro f1 score\n",
        "import json\n",
        "\n",
        "result = \"model\\ttask\\tlayer\\tacc\\tf1_macro\\n\"\n",
        "results = {}\n",
        "task = \"coref\"\n",
        "\n",
        "for model in models:\n",
        "  results[model] = {}\n",
        "  results[model][task] = {}\n",
        "  for n_layers in num_layers:\n",
        "    results[model][task][n_layers] = {}\n",
        "    create_jiant_task_config(task)\n",
        "    probing_output = run_probing_task(task, model, n_layers)\n",
        "    acc = str(probing_output[task][\"metrics\"][\"minor\"][\"acc\"])\n",
        "    f1_macro = str(probing_output[task][\"metrics\"][\"minor\"][\"f1_macro\"])\n",
        "    result += model + \"\\t\" + task + \"\\t\" + str(n_layers) + \"\\t\" + acc + \"\\t\" + f1_macro + \"\\n\"\n",
        "    results[model][task][n_layers]['acc'] = acc\n",
        "    results[model][task][n_layers]['f1_macro'] = f1_macro\n",
        "\n",
        "with open(f'../tasks/results/{task}.json', 'w') as f:\n",
        "  json.dump(results, f)\n",
        "\n",
        "for model in models:\n",
        "  plot_task(model, task, model_to_linestyle[model])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lwe2FmANkzPt"
      },
      "source": [
        "# Probe SUP-SQUAD task with bert-base and bert-squad and plot macro f1 score\n",
        "import json\n",
        "\n",
        "result = \"model\\ttask\\tlayer\\tacc\\tf1_macro\\n\"\n",
        "results = {}\n",
        "task = \"sup-squad\"\n",
        "\n",
        "for model in models:\n",
        "  results[model] = {}\n",
        "  results[model][task] = {}\n",
        "  for n_layers in num_layers:\n",
        "    results[model][task][n_layers] = {}\n",
        "    create_jiant_task_config(task)\n",
        "    probing_output = run_probing_task(task, model, n_layers)\n",
        "    acc = str(probing_output[task][\"metrics\"][\"minor\"][\"acc\"])\n",
        "    f1_macro = str(probing_output[task][\"metrics\"][\"minor\"][\"f1_macro\"])\n",
        "    result += model + \"\\t\" + task + \"\\t\" + str(n_layers) + \"\\t\" + acc + \"\\t\" + f1_macro + \"\\n\"\n",
        "    results[model][task][n_layers]['acc'] = acc\n",
        "    results[model][task][n_layers]['f1_macro'] = f1_macro\n",
        "\n",
        "with open(f'../tasks/results/{task}.json', 'w') as f:\n",
        "  json.dump(results, f)\n",
        "\n",
        "for model in models:\n",
        "  plot_task(model, task, model_to_linestyle[model])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Probe QUES task with bert-base and bert-squad and plot macro f1 score to compare the models\n",
        "import json\n",
        "\n",
        "result = \"model\\ttask\\tlayer\\tacc\\tf1_macro\\n\"\n",
        "results = {}\n",
        "task = \"ques\"\n",
        "\n",
        "for model in models:\n",
        "  results[model] = {}\n",
        "  results[model][task] = {}\n",
        "  for n_layers in num_layers:\n",
        "    results[model][task][n_layers] = {}\n",
        "    create_jiant_task_config(task)\n",
        "    probing_output = run_probing_task(task, model, n_layers)\n",
        "    acc = str(probing_output[task][\"metrics\"][\"minor\"][\"acc\"])\n",
        "    f1_macro = str(probing_output[task][\"metrics\"][\"minor\"][\"f1_macro\"])\n",
        "    result += model + \"\\t\" + task + \"\\t\" + str(n_layers) + \"\\t\" + acc + \"\\t\" + f1_macro + \"\\n\"\n",
        "    results[model][task][n_layers]['acc'] = acc\n",
        "    results[model][task][n_layers]['f1_macro'] = f1_macro\n",
        "\n",
        "with open(f'../tasks/results/{task}.json', 'w') as f:\n",
        "  json.dump(results, f)\n",
        "\n",
        "for model in models:\n",
        "  plot_task(model, task, model_to_linestyle[model])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Probe SUP-BABI task with bert-base and bert-squad and plot macro f1 score\n",
        "import json\n",
        "\n",
        "result = \"model\\ttask\\tlayer\\tacc\\tf1_macro\\n\"\n",
        "results = {}\n",
        "task = \"sup-babi\"\n",
        "\n",
        "for model in models:\n",
        "  results[model] = {}\n",
        "  results[model][task] = {}\n",
        "  for n_layers in num_layers:\n",
        "    results[model][task][n_layers] = {}\n",
        "    create_jiant_task_config(task)\n",
        "    probing_output = run_probing_task(task, model, n_layers)\n",
        "    acc = str(probing_output[task][\"metrics\"][\"minor\"][\"acc\"])\n",
        "    f1_macro = str(probing_output[task][\"metrics\"][\"minor\"][\"f1_macro\"])\n",
        "    result += model + \"\\t\" + task + \"\\t\" + str(n_layers) + \"\\t\" + acc + \"\\t\" + f1_macro + \"\\n\"\n",
        "    results[model][task][n_layers]['acc'] = acc\n",
        "    results[model][task][n_layers]['f1_macro'] = f1_macro\n",
        "\n",
        "with open(f'../tasks/results/{task}.json', 'w') as f:\n",
        "  json.dump(results, f)\n",
        "\n",
        "for model in models:\n",
        "  plot_task(model, task, model_to_linestyle[model])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ciTvHGbvk1du"
      },
      "source": [
        "# Probe SUP-HOTPOT task with bert-base and bert-squad and plot macro f1 score to compare the models\n",
        "import json\n",
        "\n",
        "result = \"model\\ttask\\tlayer\\tacc\\tf1_macro\\n\"\n",
        "results = {}\n",
        "task = \"sup-hotpot\"\n",
        "\n",
        "for model in models:\n",
        "  results[model] = {}\n",
        "  results[model][task] = {}\n",
        "  for n_layers in num_layers:\n",
        "    results[model][task][n_layers] = {}\n",
        "    create_jiant_task_config(task)\n",
        "    probing_output = run_probing_task(task, model, n_layers)\n",
        "    acc = str(probing_output[task][\"metrics\"][\"minor\"][\"acc\"])\n",
        "    f1_macro = str(probing_output[task][\"metrics\"][\"minor\"][\"f1_macro\"])\n",
        "    result += model + \"\\t\" + task + \"\\t\" + str(n_layers) + \"\\t\" + acc + \"\\t\" + f1_macro + \"\\n\"\n",
        "    results[model][task][n_layers]['acc'] = acc\n",
        "    results[model][task][n_layers]['f1_macro'] = f1_macro\n",
        "\n",
        "with open(f'../tasks/results/{task}.json', 'w') as f:\n",
        "  json.dump(results, f)\n",
        "\n",
        "for model in models:\n",
        "  plot_task(model, task, model_to_linestyle[model])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80d4GNwNxZhl"
      },
      "source": [
        "# **2. Experiment with BERT (todo: which type?) trained on (todo: which dataset?) dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xeIGu1uYxjnI"
      },
      "source": [
        "# **3. Experiment with T5 model fine-tuned on Squad**"
      ]
    }
  ]
}