{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# **Visualization - How Does BERT Answer Questions?**\n",
    "In this notebook, we will carry out the following badges:\n",
    "\n",
    "1.   Experiment with another model:\n",
    "  *   Visualize hidden states of a T5 model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import T5ForConditionalGeneration, AutoTokenizer\n",
    "\n",
    "from visualization.sample_wrapper import QASample\n",
    "from visualization.data_utils import read_squad_example, QAInputFeatures, SquadExample, convert_qa_example_to_features, whitespace_tokenize\n",
    "from utils_T5 import *"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **1. Load pretrained model and prepare the sample**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# Loading finetuned T5 model and sample QA Task\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"../models/t5_finetuned_squad_v1\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\n",
    "        \"../models/t5_finetuned_squad_v1\",\n",
    "        cache_dir=\"/cache\",\n",
    ")\n",
    "\n",
    "psample = QASample.from_json_file(\"./visualization/samples/sample_paper_squad.json\")\n",
    "psample: SquadExample = read_squad_example(psample)\n",
    "pfeatures = convert_qa_example_to_features(\n",
    "            example=psample,\n",
    "            tokenizer=tokenizer,\n",
    "            max_seq_length=384,\n",
    "            doc_stride=128,\n",
    "            max_query_length=64,\n",
    "            is_training=False,\n",
    "        )\n",
    "\n",
    "# prepare sample for input to model\n",
    "sample = json.load(open(\"./visualization/samples/sample_paper_squad.json\"))# QASample.from_json_file(\"./visualization/samples/sample_paper_squad.json\")\n",
    "sample[\"answers\"] = {}\n",
    "sample[\"answers\"]['text'] = [sample['answer']]\n",
    "sample = add_eos_to_examples(sample)\n",
    "answer_start = sample[\"context\"].lower().find(sample[\"answers\"]['text'][0].lower())\n",
    "\n",
    "features = tokenizer([sample['input_text']], return_tensors='pt')\n",
    "features[\"tokens\"] = tokenizer.convert_ids_to_tokens(features[\"input_ids\"][0])\n",
    "print(pfeatures.sup_ids)\n",
    "print(sample[\"answers\"]['text'])\n",
    "print(features[\"tokens\"], features[\"input_ids\"][0])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[(14, 47)]\n",
      "['detention']\n",
      "['▁question', ':', '▁What', '▁is', '▁', 'a', '▁common', '▁punishment', '▁in', '▁the', '▁UK', '▁and', '▁Ireland', '?', '▁context', ':', '▁', 'Currently', '▁de', 'tention', '▁is', '▁one', '▁of', '▁the', '▁most', '▁common', '▁pun', '-', '▁', 'ish', 'ments', '▁in', '▁schools', '▁in', '▁the', '▁United', '▁States', ',', '▁the', '▁UK', ',', '▁I', 're', '-', '▁land', ',', '▁Singapore', '▁and', '▁other', '▁countries', '.', '▁It', '▁requires', '▁the', '▁pupil', '▁to', '▁remain', '▁in', '▁school', '▁at', '▁', 'a', '▁given', '▁time', '▁in', '▁the', '▁school', '▁day', '▁(', 'such', '▁as', '▁lunch', ',', '▁rece', 's', 's', '▁or', '▁after', '▁school', ');', '▁or', '▁even', '▁to', '▁attend', '▁school', '▁on', '▁', 'a', '▁non', '-', 'school', '▁day', ',', '▁', 'e', '.', 'g', '.', '▁', \"'\", 'S', 'atur', 'day', '▁de', 'tention', \"'\", '▁held', '▁at', '▁some', '▁schools', '.', '▁', 'During', '▁de', 'tention', ',', '▁students', '▁normally', '▁have', '▁to', '▁sit', '▁in', '▁', 'a', '▁classroom', '▁and', '▁do', '▁work', ',', '▁write', '▁lines', '▁or', '▁', 'a', '▁punishment', '▁essay', ',', '▁or', '▁sit', '▁quietly', '.', '</s>'] tensor([  822,    10,   363,    19,     3,     9,  1017, 19372,    16,     8,\n",
      "         1270,    11,  5316,    58,  2625,    10,     3,  8212,    20,  9174,\n",
      "           19,    80,    13,     8,   167,  1017,  4930,    18,     3,  1273,\n",
      "         4128,    16,  2061,    16,     8,   907,  1323,     6,     8,  1270,\n",
      "            6,    27,    60,    18,  1322,     6,  6243,    11,   119,  1440,\n",
      "            5,    94,  2311,     8, 26913,    12,  2367,    16,   496,    44,\n",
      "            3,     9,   787,    97,    16,     8,   496,   239,    41,  4415,\n",
      "           38,  3074,     6,  9088,     7,     7,    42,   227,   496,  3670,\n",
      "           42,   237,    12,  2467,   496,    30,     3,     9,   529,    18,\n",
      "         6646,   239,     6,     3,    15,     5,   122,     5,     3,    31,\n",
      "          134,  6010,  1135,    20,  9174,    31,  1213,    44,   128,  2061,\n",
      "            5,     3,  2092,    20,  9174,     6,   481,  4929,    43,    12,\n",
      "         2561,    16,     3,     9,  4858,    11,   103,   161,     6,  1431,\n",
      "         2356,    42,     3,     9, 19372,  2257,     6,    42,  2561, 19965,\n",
      "            5,     1])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **2. Predict answer from the model and extract answer/question/support-fact token ids/ranges**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# Generate answer and get hidden states\n",
    "outs = model.generate(input_ids=features[\"input_ids\"],\n",
    "                        attention_mask=features[\"attention_mask\"],\n",
    "                        max_length=16,\n",
    "                        early_stopping=True,\n",
    "                        output_hidden_states=True,\n",
    "                        return_dict_in_generate=True)\n",
    "\n",
    "answer = [tokenizer.decode(ids, skip_special_tokens=True, clean_up_tokenization_spaces=True) for ids in outs.sequences]\n",
    "encoder_hstates = outs.encoder_hidden_states\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "from nltk import tokenize\n",
    "# get token ids for answer/question/supporting facts\n",
    "ids = features[\"input_ids\"][0].int().numpy()\n",
    "answer_index = [np.where(ids==o.numpy())[0][0] for o in outs.sequences[0] if o != 0 and o != 1] # get only the first occurence of the answer.\n",
    "# question index range\n",
    "question_end_index = [i for i, e in enumerate(features[\"tokens\"]) if 'context' in e][0]\n",
    "question_index_range = [0, question_end_index]\n",
    "# support fact index range for squad sample\n",
    "sentences =  tokenize.sent_tokenize(sample['context'])\n",
    "support_sentence = [s for s in sentences if answer[0] in s][0]\n",
    "sup_sent_ids = tokenizer(support_sentence, return_tensors='pt')['input_ids'][0].numpy()[:-1] # remove end token\n",
    "sup_start = [x for x in range(len(ids)) if np.all(ids[x:x+len(sup_sent_ids)] == sup_sent_ids)][0]\n",
    "support_facts_range = [sup_start, sup_start+len(sup_sent_ids)-1]\n",
    "print(answer, answer_index, question_index_range, support_facts_range)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['detention'] [18, 19] [0, 14] [16, 50]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-36-caa89a9949db>:12: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  sup_start = [x for x in range(len(ids)) if np.all(ids[x:x+len(sup_sent_ids)] == sup_sent_ids)][0]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **3. Visualize hidden states with PCA**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from visualization.plotting import Token2DVector, TokenPlotter, TokenLabel\n",
    "from typing import List\n",
    "tokens = features[\"tokens\"]\n",
    "sup_facts_pos = support_facts_range\n",
    "for index, layer in enumerate(encoder_hstates):\n",
    "    token_vectors: List = layer[0][:len(tokens)]\n",
    "    reduction = PCA(n_components=2)\n",
    "    layer_reduced = reduction.fit_transform(token_vectors.detach().numpy()).transpose()\n",
    "\n",
    "    token_vectors = []\n",
    "    for token_index, value in enumerate(layer_reduced[0]):  # iterate over x-values\n",
    "        label = TokenLabel.DEFAULT\n",
    "        is_supporting_fact_token = False\n",
    "        if sup_fact_pos[0] <= token_index <= sup_fact_pos[1]:\n",
    "            is_supporting_fact_token = True\n",
    "        if is_supporting_fact_token:\n",
    "            label = TokenLabel.SUP_FACT\n",
    "        if token_index >= answer_index[0] and token_index <= answer_index[1]:\n",
    "            label = TokenLabel.PREDICTION\n",
    "        elif token_index >= question_index[0] and token_index <= question_index[1]:\n",
    "            label = TokenLabel.QUESTION\n",
    "\n",
    "        token_vector = Token2DVector(x=value,\n",
    "                                    y=layer_reduced[1][token_index],\n",
    "                                    token=tokens[token_index],\n",
    "                                    label=label)\n",
    "        token_vectors.append(token_vector)\n",
    "    plot_title = \"Layer {}\".format(index)\n",
    "    token_plotter = TokenPlotter(vectors=token_vectors,\n",
    "                                    title=plot_title,\n",
    "                                    output_path=None)\n",
    "    token_plotter.plot()"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "'int' object is not subscriptable",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-5371ca262e49>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mis_supporting_fact_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0msup_fact_pos\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msup_facts_pos\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0msup_fact_pos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mtoken_index\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0msup_fact_pos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m                 \u001b[0mis_supporting_fact_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not subscriptable"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b8672627447c0f0535055ab3d8fd53b536c1a6bd2e3f5068fe9589aa046dabaa"
  },
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit ('venv': venv)",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}